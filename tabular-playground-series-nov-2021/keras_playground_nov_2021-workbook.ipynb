{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "v7L8ZT2almwl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib as mplt\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_curve, roc_auc_score, f1_score\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import Adamax\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "lbqW8TTGp4hP"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "NBgD1CYTqTsR"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv('Data/train.csv')\n",
        "test_data = pd.read_csv('Data/test.csv')\n",
        "submission_file = pd.read_csv('Data/sample_submission.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "uQl7kDa8qvxK"
      },
      "outputs": [],
      "source": [
        "train_data.drop(columns=['id'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlErXZC-qx2w",
        "outputId": "a6a3b783-a4b2-4d56-a2ae-d5933c79fdce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(600000, 101)\n",
            "(540000, 101)\n"
          ]
        }
      ],
      "source": [
        "print(train_data.shape)\n",
        "print(test_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "FKLnnrkmq0OG"
      },
      "outputs": [],
      "source": [
        "X, y = train_data.drop(columns = ['target']), train_data['target']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ilm-eJVq3Sz",
        "outputId": "dc1c720b-d5a9-49e5-dbf3-1fa190ddf9a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dtypes = train_data.dtypes\n",
        "dtypes = dtypes[dtypes != 'object']\n",
        "features = list(set(dtypes.index) - set(['target']))\n",
        "\n",
        "len(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "YTGSonwyq6fu",
        "outputId": "87363f93-8c1f-492e-a2c4-225c4596344f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f0</th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "      <th>f6</th>\n",
              "      <th>f7</th>\n",
              "      <th>f8</th>\n",
              "      <th>f9</th>\n",
              "      <th>...</th>\n",
              "      <th>f91</th>\n",
              "      <th>f92</th>\n",
              "      <th>f93</th>\n",
              "      <th>f94</th>\n",
              "      <th>f95</th>\n",
              "      <th>f96</th>\n",
              "      <th>f97</th>\n",
              "      <th>f98</th>\n",
              "      <th>f99</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.106643</td>\n",
              "      <td>3.59437</td>\n",
              "      <td>132.8040</td>\n",
              "      <td>3.18428</td>\n",
              "      <td>0.081971</td>\n",
              "      <td>1.18859</td>\n",
              "      <td>3.73238</td>\n",
              "      <td>2.266270</td>\n",
              "      <td>2.09959</td>\n",
              "      <td>0.012330</td>\n",
              "      <td>...</td>\n",
              "      <td>1.09862</td>\n",
              "      <td>0.013331</td>\n",
              "      <td>-0.011715</td>\n",
              "      <td>0.052759</td>\n",
              "      <td>0.065400</td>\n",
              "      <td>4.211250</td>\n",
              "      <td>1.97877</td>\n",
              "      <td>0.085974</td>\n",
              "      <td>0.240496</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.125021</td>\n",
              "      <td>1.67336</td>\n",
              "      <td>76.5336</td>\n",
              "      <td>3.37825</td>\n",
              "      <td>0.099400</td>\n",
              "      <td>5.09366</td>\n",
              "      <td>1.27562</td>\n",
              "      <td>-0.471318</td>\n",
              "      <td>4.54594</td>\n",
              "      <td>0.037706</td>\n",
              "      <td>...</td>\n",
              "      <td>3.46017</td>\n",
              "      <td>0.017054</td>\n",
              "      <td>0.124863</td>\n",
              "      <td>0.154064</td>\n",
              "      <td>0.606848</td>\n",
              "      <td>-0.267928</td>\n",
              "      <td>2.57786</td>\n",
              "      <td>-0.020877</td>\n",
              "      <td>0.024719</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.036330</td>\n",
              "      <td>1.49747</td>\n",
              "      <td>233.5460</td>\n",
              "      <td>2.19435</td>\n",
              "      <td>0.026914</td>\n",
              "      <td>3.12694</td>\n",
              "      <td>5.05687</td>\n",
              "      <td>3.849460</td>\n",
              "      <td>1.80187</td>\n",
              "      <td>0.056995</td>\n",
              "      <td>...</td>\n",
              "      <td>4.88300</td>\n",
              "      <td>0.085222</td>\n",
              "      <td>0.032396</td>\n",
              "      <td>0.116092</td>\n",
              "      <td>-0.001688</td>\n",
              "      <td>-0.520069</td>\n",
              "      <td>2.14112</td>\n",
              "      <td>0.124464</td>\n",
              "      <td>0.148209</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.014077</td>\n",
              "      <td>0.24600</td>\n",
              "      <td>779.9670</td>\n",
              "      <td>1.89064</td>\n",
              "      <td>0.006948</td>\n",
              "      <td>1.53112</td>\n",
              "      <td>2.69800</td>\n",
              "      <td>4.517330</td>\n",
              "      <td>4.50332</td>\n",
              "      <td>0.123494</td>\n",
              "      <td>...</td>\n",
              "      <td>3.47439</td>\n",
              "      <td>-0.017103</td>\n",
              "      <td>-0.008100</td>\n",
              "      <td>0.062013</td>\n",
              "      <td>0.041193</td>\n",
              "      <td>0.511657</td>\n",
              "      <td>1.96860</td>\n",
              "      <td>0.040017</td>\n",
              "      <td>0.044873</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.003259</td>\n",
              "      <td>3.71542</td>\n",
              "      <td>156.1280</td>\n",
              "      <td>2.14772</td>\n",
              "      <td>0.018284</td>\n",
              "      <td>2.09859</td>\n",
              "      <td>4.15492</td>\n",
              "      <td>-0.038236</td>\n",
              "      <td>3.37145</td>\n",
              "      <td>0.034166</td>\n",
              "      <td>...</td>\n",
              "      <td>1.91059</td>\n",
              "      <td>-0.042943</td>\n",
              "      <td>0.105616</td>\n",
              "      <td>0.125072</td>\n",
              "      <td>0.037509</td>\n",
              "      <td>1.043790</td>\n",
              "      <td>1.07481</td>\n",
              "      <td>-0.012819</td>\n",
              "      <td>0.072798</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 101 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         f0       f1        f2       f3        f4       f5       f6        f7  \\\n",
              "0  0.106643  3.59437  132.8040  3.18428  0.081971  1.18859  3.73238  2.266270   \n",
              "1  0.125021  1.67336   76.5336  3.37825  0.099400  5.09366  1.27562 -0.471318   \n",
              "2  0.036330  1.49747  233.5460  2.19435  0.026914  3.12694  5.05687  3.849460   \n",
              "3 -0.014077  0.24600  779.9670  1.89064  0.006948  1.53112  2.69800  4.517330   \n",
              "4 -0.003259  3.71542  156.1280  2.14772  0.018284  2.09859  4.15492 -0.038236   \n",
              "\n",
              "        f8        f9  ...      f91       f92       f93       f94       f95  \\\n",
              "0  2.09959  0.012330  ...  1.09862  0.013331 -0.011715  0.052759  0.065400   \n",
              "1  4.54594  0.037706  ...  3.46017  0.017054  0.124863  0.154064  0.606848   \n",
              "2  1.80187  0.056995  ...  4.88300  0.085222  0.032396  0.116092 -0.001688   \n",
              "3  4.50332  0.123494  ...  3.47439 -0.017103 -0.008100  0.062013  0.041193   \n",
              "4  3.37145  0.034166  ...  1.91059 -0.042943  0.105616  0.125072  0.037509   \n",
              "\n",
              "        f96      f97       f98       f99  target  \n",
              "0  4.211250  1.97877  0.085974  0.240496       0  \n",
              "1 -0.267928  2.57786 -0.020877  0.024719       0  \n",
              "2 -0.520069  2.14112  0.124464  0.148209       0  \n",
              "3  0.511657  1.96860  0.040017  0.044873       0  \n",
              "4  1.043790  1.07481 -0.012819  0.072798       1  \n",
              "\n",
              "[5 rows x 101 columns]"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "EnnOgS9Rrfol",
        "outputId": "c4da73b4-1a40-4732-aacb-7936c1dd4892"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f0</th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "      <th>f6</th>\n",
              "      <th>f7</th>\n",
              "      <th>f8</th>\n",
              "      <th>f9</th>\n",
              "      <th>f10</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.306508</td>\n",
              "      <td>2.497590</td>\n",
              "      <td>306.644536</td>\n",
              "      <td>2.647901</td>\n",
              "      <td>0.177850</td>\n",
              "      <td>2.556832</td>\n",
              "      <td>2.699650</td>\n",
              "      <td>2.571593</td>\n",
              "      <td>2.538273</td>\n",
              "      <td>0.134370</td>\n",
              "      <td>2.579987</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.522450</td>\n",
              "      <td>1.554018</td>\n",
              "      <td>551.743893</td>\n",
              "      <td>1.544529</td>\n",
              "      <td>0.417488</td>\n",
              "      <td>1.562527</td>\n",
              "      <td>1.564000</td>\n",
              "      <td>1.549361</td>\n",
              "      <td>1.532988</td>\n",
              "      <td>0.421892</td>\n",
              "      <td>1.604389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-3.797450</td>\n",
              "      <td>-1.223960</td>\n",
              "      <td>-1842.530000</td>\n",
              "      <td>-1.368560</td>\n",
              "      <td>-3.206210</td>\n",
              "      <td>-1.169770</td>\n",
              "      <td>-1.059310</td>\n",
              "      <td>-1.281970</td>\n",
              "      <td>-1.242020</td>\n",
              "      <td>-2.577840</td>\n",
              "      <td>-1.309730</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.026222</td>\n",
              "      <td>1.186237</td>\n",
              "      <td>43.573400</td>\n",
              "      <td>1.442028</td>\n",
              "      <td>0.019709</td>\n",
              "      <td>1.261038</td>\n",
              "      <td>1.385820</td>\n",
              "      <td>1.333848</td>\n",
              "      <td>1.292163</td>\n",
              "      <td>0.019563</td>\n",
              "      <td>1.205920</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.097788</td>\n",
              "      <td>2.516500</td>\n",
              "      <td>133.626000</td>\n",
              "      <td>2.634130</td>\n",
              "      <td>0.061586</td>\n",
              "      <td>2.590425</td>\n",
              "      <td>2.801255</td>\n",
              "      <td>2.557985</td>\n",
              "      <td>2.475880</td>\n",
              "      <td>0.058752</td>\n",
              "      <td>2.527070</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.397184</td>\n",
              "      <td>3.787630</td>\n",
              "      <td>302.262250</td>\n",
              "      <td>3.907640</td>\n",
              "      <td>0.112712</td>\n",
              "      <td>3.813662</td>\n",
              "      <td>3.996913</td>\n",
              "      <td>3.823450</td>\n",
              "      <td>3.804360</td>\n",
              "      <td>0.101046</td>\n",
              "      <td>3.956182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>8.781500</td>\n",
              "      <td>6.226720</td>\n",
              "      <td>6119.280000</td>\n",
              "      <td>6.521150</td>\n",
              "      <td>8.265470</td>\n",
              "      <td>6.515070</td>\n",
              "      <td>6.586780</td>\n",
              "      <td>6.258770</td>\n",
              "      <td>6.389670</td>\n",
              "      <td>7.078460</td>\n",
              "      <td>6.508760</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  f0             f1             f2             f3  \\\n",
              "count  600000.000000  600000.000000  600000.000000  600000.000000   \n",
              "mean        0.306508       2.497590     306.644536       2.647901   \n",
              "std         0.522450       1.554018     551.743893       1.544529   \n",
              "min        -3.797450      -1.223960   -1842.530000      -1.368560   \n",
              "25%         0.026222       1.186237      43.573400       1.442028   \n",
              "50%         0.097788       2.516500     133.626000       2.634130   \n",
              "75%         0.397184       3.787630     302.262250       3.907640   \n",
              "max         8.781500       6.226720    6119.280000       6.521150   \n",
              "\n",
              "                  f4             f5             f6             f7  \\\n",
              "count  600000.000000  600000.000000  600000.000000  600000.000000   \n",
              "mean        0.177850       2.556832       2.699650       2.571593   \n",
              "std         0.417488       1.562527       1.564000       1.549361   \n",
              "min        -3.206210      -1.169770      -1.059310      -1.281970   \n",
              "25%         0.019709       1.261038       1.385820       1.333848   \n",
              "50%         0.061586       2.590425       2.801255       2.557985   \n",
              "75%         0.112712       3.813662       3.996913       3.823450   \n",
              "max         8.265470       6.515070       6.586780       6.258770   \n",
              "\n",
              "                  f8             f9            f10  \n",
              "count  600000.000000  600000.000000  600000.000000  \n",
              "mean        2.538273       0.134370       2.579987  \n",
              "std         1.532988       0.421892       1.604389  \n",
              "min        -1.242020      -2.577840      -1.309730  \n",
              "25%         1.292163       0.019563       1.205920  \n",
              "50%         2.475880       0.058752       2.527070  \n",
              "75%         3.804360       0.101046       3.956182  \n",
              "max         6.389670       7.078460       6.508760  "
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.iloc[:,:11].describe() # f2 f35"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f30</th>\n",
              "      <th>f31</th>\n",
              "      <th>f32</th>\n",
              "      <th>f33</th>\n",
              "      <th>f34</th>\n",
              "      <th>f35</th>\n",
              "      <th>f36</th>\n",
              "      <th>f37</th>\n",
              "      <th>f38</th>\n",
              "      <th>f39</th>\n",
              "      <th>f40</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.125638</td>\n",
              "      <td>0.107798</td>\n",
              "      <td>0.170434</td>\n",
              "      <td>0.054776</td>\n",
              "      <td>2.463409</td>\n",
              "      <td>55.698848</td>\n",
              "      <td>1.769510</td>\n",
              "      <td>2.595079</td>\n",
              "      <td>2.410926</td>\n",
              "      <td>0.593497</td>\n",
              "      <td>2.646091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.296563</td>\n",
              "      <td>0.412291</td>\n",
              "      <td>0.440615</td>\n",
              "      <td>0.109661</td>\n",
              "      <td>1.566572</td>\n",
              "      <td>130.132892</td>\n",
              "      <td>1.785712</td>\n",
              "      <td>1.562506</td>\n",
              "      <td>1.572078</td>\n",
              "      <td>0.951892</td>\n",
              "      <td>1.567926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-2.025710</td>\n",
              "      <td>-3.273000</td>\n",
              "      <td>-1.954520</td>\n",
              "      <td>-3.318420</td>\n",
              "      <td>-1.517720</td>\n",
              "      <td>-397.004000</td>\n",
              "      <td>-2.888940</td>\n",
              "      <td>-1.210370</td>\n",
              "      <td>-1.181050</td>\n",
              "      <td>-2.611230</td>\n",
              "      <td>-1.098840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.018166</td>\n",
              "      <td>0.005452</td>\n",
              "      <td>0.018785</td>\n",
              "      <td>0.019266</td>\n",
              "      <td>1.200895</td>\n",
              "      <td>6.671687</td>\n",
              "      <td>0.308010</td>\n",
              "      <td>1.306240</td>\n",
              "      <td>1.071610</td>\n",
              "      <td>0.045389</td>\n",
              "      <td>1.315937</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.057954</td>\n",
              "      <td>0.046137</td>\n",
              "      <td>0.063493</td>\n",
              "      <td>0.050234</td>\n",
              "      <td>2.418740</td>\n",
              "      <td>21.222700</td>\n",
              "      <td>1.441415</td>\n",
              "      <td>2.599130</td>\n",
              "      <td>2.392395</td>\n",
              "      <td>0.189330</td>\n",
              "      <td>2.760870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.103623</td>\n",
              "      <td>0.090480</td>\n",
              "      <td>0.114393</td>\n",
              "      <td>0.081535</td>\n",
              "      <td>3.746650</td>\n",
              "      <td>48.320200</td>\n",
              "      <td>2.714503</td>\n",
              "      <td>3.884860</td>\n",
              "      <td>3.697963</td>\n",
              "      <td>0.790835</td>\n",
              "      <td>3.928752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>5.451270</td>\n",
              "      <td>8.646590</td>\n",
              "      <td>5.176510</td>\n",
              "      <td>5.487420</td>\n",
              "      <td>6.582410</td>\n",
              "      <td>1464.470000</td>\n",
              "      <td>13.029000</td>\n",
              "      <td>6.255900</td>\n",
              "      <td>6.306940</td>\n",
              "      <td>8.881050</td>\n",
              "      <td>6.306290</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 f30            f31            f32            f33  \\\n",
              "count  600000.000000  600000.000000  600000.000000  600000.000000   \n",
              "mean        0.125638       0.107798       0.170434       0.054776   \n",
              "std         0.296563       0.412291       0.440615       0.109661   \n",
              "min        -2.025710      -3.273000      -1.954520      -3.318420   \n",
              "25%         0.018166       0.005452       0.018785       0.019266   \n",
              "50%         0.057954       0.046137       0.063493       0.050234   \n",
              "75%         0.103623       0.090480       0.114393       0.081535   \n",
              "max         5.451270       8.646590       5.176510       5.487420   \n",
              "\n",
              "                 f34            f35            f36            f37  \\\n",
              "count  600000.000000  600000.000000  600000.000000  600000.000000   \n",
              "mean        2.463409      55.698848       1.769510       2.595079   \n",
              "std         1.566572     130.132892       1.785712       1.562506   \n",
              "min        -1.517720    -397.004000      -2.888940      -1.210370   \n",
              "25%         1.200895       6.671687       0.308010       1.306240   \n",
              "50%         2.418740      21.222700       1.441415       2.599130   \n",
              "75%         3.746650      48.320200       2.714503       3.884860   \n",
              "max         6.582410    1464.470000      13.029000       6.255900   \n",
              "\n",
              "                 f38            f39            f40  \n",
              "count  600000.000000  600000.000000  600000.000000  \n",
              "mean        2.410926       0.593497       2.646091  \n",
              "std         1.572078       0.951892       1.567926  \n",
              "min        -1.181050      -2.611230      -1.098840  \n",
              "25%         1.071610       0.045389       1.315937  \n",
              "50%         2.392395       0.189330       2.760870  \n",
              "75%         3.697963       0.790835       3.928752  \n",
              "max         6.306940       8.881050       6.306290  "
            ]
          },
          "execution_count": 71,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.iloc[:,30:41].describe() #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyGhcp2StNcg",
        "outputId": "43fcef47-42c5-40e8-ef57-fdd86de2ce55"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count    600000.000000\n",
              "mean          0.506010\n",
              "std           0.499964\n",
              "min           0.000000\n",
              "25%           0.000000\n",
              "50%           1.000000\n",
              "75%           1.000000\n",
              "max           1.000000\n",
              "Name: target, dtype: float64"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "jMx41cxq0q25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "check for null value in X: 0\n",
            "check for null value in y: 0\n"
          ]
        }
      ],
      "source": [
        "X = X.astype(float)\n",
        "y = y.astype(int)\n",
        "\n",
        "X.fillna(X.mean(), inplace=True)\n",
        "y.fillna(y.mean(), inplace=True)\n",
        "\n",
        "print(f'check for null value in X: {X.isnull().sum().sum()}')\n",
        "print(f'check for null value in y: {y.isnull().sum().sum()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "X_test = X_test.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "# MinMaxScaler and RobustScaler do not work well\n",
        "\n",
        "num_cols = X_train.select_dtypes(['integer', 'float']).columns\n",
        "\n",
        "X_train = pd.DataFrame(scaler.fit_transform(X_train[num_cols]), columns=num_cols)\n",
        "X_test = pd.DataFrame(scaler.fit_transform(X_test[num_cols]), columns=num_cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Common Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_keras_sequential_model(optimizer='adam', init='glorot_uniform'):\n",
        "\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(128, input_dim=100, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(64, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(32, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(16, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\t\n",
        "\tmodel.add(Dense(1, kernel_initializer=init, activation='sigmoid'))\n",
        "\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "\n",
        "\treturn model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "def report_results(model_name, y_test, y_train, grid_search_model, grid_search_results):\n",
        "\n",
        "\t# summarize results\n",
        "\tprint(\"Best: %f using %s\" % (grid_search_results.best_score_, grid_search_results.best_params_))\n",
        "\tmeans = grid_search_results.cv_results_['mean_test_score']\n",
        "\tstds = grid_search_results.cv_results_['std_test_score']\n",
        "\tparams = grid_search_results.cv_results_['params']\n",
        "\n",
        "\tfor mean, stdev, param in zip(means, stds, params):\n",
        "\t\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
        "\n",
        "\ty_hat_train = grid_search_model.predict(X_train)\n",
        "\ty_hat_test = grid_search_model.predict(X_test)\n",
        "\n",
        "\ttrain_score = accuracy_score(y_train, y_hat_train, normalize=False)\n",
        "\tprint(f'trian score: {train_score / y_train.shape[0]}')\n",
        "\n",
        "\ttest_score = accuracy_score(y_test, y_hat_test, normalize=False)\n",
        "\tprint(f'test score: {test_score / y_test.shape[0]}')\n",
        "\n",
        "\tprecision_train_score = precision_score(y_train, y_hat_train) * 100\n",
        "\tprecision_test_score = precision_score(y_test, y_hat_test) * 100\n",
        "\n",
        "\trecall_train_score = recall_score(y_train, y_hat_train) * 100\n",
        "\trecall_test_score = recall_score(y_test, y_hat_test) * 100\n",
        "\n",
        "\tf1_train_score = f1_score(y_train, y_hat_train) * 100\n",
        "\tf1_test_score = f1_score(y_test, y_hat_test) * 100\n",
        "\n",
        "\tauc_train_score = roc_auc_score(y_train, y_hat_train) * 100\n",
        "\tauc_test_score = roc_auc_score(y_test, y_hat_test) * 100\n",
        "\n",
        "\tprint(\"Precision = {:.2f}% , recall = {:.2f}% and f1_score={:.2f}% of the % model on the training data.\".format(precision_train_score, recall_train_score, f1_train_score, model_name))\n",
        "\tprint(\"Precision = {:.2f}% , recall = {:.2f}% and f1_score={:.2f}% of the % model on the validation data.\".format(precision_test_score, recall_test_score, f1_test_score, model_name))\n",
        "\tprint(\"ROC_AUC Score = {:.2f}%  of the % model on the training data.\".format(auc_train_score, model_name))\n",
        "\tprint(\"ROC_AUC Score = {:.2f}%  of the % model on the validation data.\".format(auc_test_score, model_name))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initial Baseline Implementation using KerasClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/v3/ph9z3jvd3h32sv82kzzmklym0000gn/T/ipykernel_847/275589273.py:35: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
            "  model = KerasClassifier(build_fn=create_model, verbose=0)\n",
            "2021-11-27 10:36:35.591631: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-27 10:36:35.591823: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-27 10:36:35.591909: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-27 10:36:35.592006: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-27 10:36:35.592516: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best: 0.730531 using {'batch_size': 2048, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.730531 (0.007175) with: {'batch_size': 2048, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "trian score: 0.7325733333333333\n",
            "test score: 0.7168\n",
            "Precision = 72.09% , recall = 76.93% and f1_score=74.43% of the % model on the training data.\n",
            "Precision = 75.05% , recall = 66.01% and f1_score=70.24% of the % model on the validation data.\n",
            "ROC_AUC Score = 73.21%  of the % model on the training data.\n",
            "ROC_AUC Score = 71.75%  of the % model on the validation data.\n"
          ]
        }
      ],
      "source": [
        "# Function to create model, required for KerasClassifier\n",
        "def create_model(optimizer='rmsprop', init='glorot_uniform'):\n",
        "\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(128, input_dim=100, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(64, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(32, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(16, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\t\n",
        "\tmodel.add(Dense(1, kernel_initializer=init, activation='sigmoid'))\n",
        "\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "\treturn model\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "\n",
        "# grid search epochs, batch size and optimizer\n",
        "#optimizers = ['rmsprop', 'adam']\n",
        "#init = ['glorot_uniform', 'normal', 'uniform']\n",
        "#epochs = [100, 150]\n",
        "#batches = [1024, 2048]\n",
        "\n",
        "optimizers = ['adam']\n",
        "init = ['glorot_uniform']\n",
        "epochs = [100] #[1000]\n",
        "batches = [2048]\n",
        "\n",
        "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "report_results('KerasClassifier', y_test, y_train, grid, grid_result)\n",
        "\n",
        "# public score: 0.74736 (427th)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tune the Hidden Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/v3/ph9z3jvd3h32sv82kzzmklym0000gn/T/ipykernel_847/4288725989.py:35: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
            "  model = KerasClassifier(build_fn=create_model, verbose=0)\n",
            "2021-11-27 11:17:50.005007: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-27 11:17:50.009467: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-27 11:17:50.011054: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-27 11:17:50.011161: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-27 11:17:50.011159: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best: 0.741067 using {'batch_size': 2048, 'epochs': 200, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.741067 (0.001884) with: {'batch_size': 2048, 'epochs': 200, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "trian score: 0.7535755555555556\n",
            "test score: 0.7431\n",
            "Precision = 75.50% , recall = 75.92% and f1_score=75.71% of the % model on the training data.\n",
            "Precision = 74.48% , recall = 74.94% and f1_score=74.71% of the % model on the validation data.\n",
            "ROC_AUC Score = 75.35%  of the % model on the training data.\n",
            "ROC_AUC Score = 74.30%  of the % model on the validation data.\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "X_test = X_test.reset_index(drop=True)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "num_cols = X_train.select_dtypes(['integer', 'float']).columns\n",
        "\n",
        "X_train = pd.DataFrame(scaler.fit_transform(X_train[num_cols]), columns=num_cols)\n",
        "X_test = pd.DataFrame(scaler.fit_transform(X_test[num_cols]), columns=num_cols)\n",
        "\n",
        "# Function to create model, required for KerasClassifier\n",
        "def create_model(optimizer='rmsprop', init='glorot_uniform'):\n",
        "\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(128, input_dim=100, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(64, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(32, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(16, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\t\n",
        "\tmodel.add(Dense(8, kernel_initializer=init, activation='relu')) # new layer\n",
        "\tmodel.add(Dropout(0.2))\t\t\n",
        "\tmodel.add(Dense(1, kernel_initializer=init, activation='sigmoid'))\n",
        "\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "\treturn model\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "\n",
        "# grid search epochs, batch size and optimizer\n",
        "optimizers = ['adam']\n",
        "init = ['glorot_uniform']\n",
        "epochs = [200]\n",
        "batches = [2048]\n",
        "\n",
        "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "report_results('KerasClassifier', y_test, y_train, grid, grid_result)\n",
        "\n",
        "# public score: ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/v3/ph9z3jvd3h32sv82kzzmklym0000gn/T/ipykernel_847/206936040.py:35: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
            "  model = KerasClassifier(build_fn=create_model, verbose=0)\n",
            "2021-11-27 11:32:26.445214: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-27 11:32:26.467839: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best: 0.741453 using {'batch_size': 2048, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.741453 (0.002087) with: {'batch_size': 2048, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "trian score: 0.7526822222222223\n",
            "test score: 0.74382\n",
            "Precision = 75.03% , recall = 76.60% and f1_score=75.81% of the % model on the training data.\n",
            "Precision = 74.23% , recall = 75.68% and f1_score=74.95% of the % model on the validation data.\n",
            "ROC_AUC Score = 75.25%  of the % model on the training data.\n",
            "ROC_AUC Score = 74.37%  of the % model on the validation data.\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "X_test = X_test.reset_index(drop=True)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "num_cols = X_train.select_dtypes(['integer', 'float']).columns\n",
        "\n",
        "X_train = pd.DataFrame(scaler.fit_transform(X_train[num_cols]), columns=num_cols)\n",
        "X_test = pd.DataFrame(scaler.fit_transform(X_test[num_cols]), columns=num_cols)\n",
        "\n",
        "# Function to create model, required for KerasClassifier\n",
        "def create_model(optimizer='rmsprop', init='glorot_uniform'):\n",
        "\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(128, input_dim=100, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(64, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(32, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(16, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\t\n",
        "\tmodel.add(Dense(8, kernel_initializer=init, activation='relu')) # new layer\n",
        "\tmodel.add(Dropout(0.2))\t\t\n",
        "\tmodel.add(Dense(1, kernel_initializer=init, activation='sigmoid'))\n",
        "\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "\treturn model\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "\n",
        "# grid search epochs, batch size and optimizer\n",
        "optimizers = ['adam']\n",
        "init = ['glorot_uniform']\n",
        "epochs = [100]\n",
        "batches = [2048]\n",
        "\n",
        "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "report_results('KerasClassifier', y_test, y_train, grid, grid_result)\n",
        "\n",
        "# public score: ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/v3/ph9z3jvd3h32sv82kzzmklym0000gn/T/ipykernel_847/2623893511.py:35: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
            "  model = KerasClassifier(build_fn=create_model, verbose=0)\n",
            "2021-11-27 11:42:28.699541: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-27 11:42:28.703401: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-27 11:42:28.705494: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-27 11:42:28.706672: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-27 11:42:28.711158: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best: 0.742567 using {'batch_size': 2048, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.742567 (0.002044) with: {'batch_size': 2048, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "trian score: 0.7526555555555555\n",
            "test score: 0.7442\n",
            "Precision = 75.13% , recall = 76.41% and f1_score=75.76% of the % model on the training data.\n",
            "Precision = 74.31% , recall = 75.62% and f1_score=74.96% of the % model on the validation data.\n",
            "ROC_AUC Score = 75.25%  of the % model on the training data.\n",
            "ROC_AUC Score = 74.40%  of the % model on the validation data.\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "X_test = X_test.reset_index(drop=True)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "num_cols = X_train.select_dtypes(['integer', 'float']).columns\n",
        "\n",
        "X_train = pd.DataFrame(scaler.fit_transform(X_train[num_cols]), columns=num_cols)\n",
        "X_test = pd.DataFrame(scaler.fit_transform(X_test[num_cols]), columns=num_cols)\n",
        "\n",
        "# Function to create model, required for KerasClassifier\n",
        "def create_model(optimizer='rmsprop', init='glorot_uniform'):\n",
        "\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(128, input_dim=100, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(64, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(32, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(16, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\t\n",
        "\t#model.add(Dense(8, kernel_initializer=init, activation='relu')) # new layer\n",
        "\t#model.add(Dropout(0.2))\t\t\n",
        "\tmodel.add(Dense(1, kernel_initializer=init, activation='sigmoid'))\n",
        "\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "\treturn model\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "\n",
        "# grid search epochs, batch size and optimizer\n",
        "optimizers = ['adam']\n",
        "init = ['glorot_uniform']\n",
        "epochs = [100]\n",
        "batches = [2048]\n",
        "\n",
        "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "report_results('KerasClassifier', y_test, y_train, grid, grid_result)\n",
        "\n",
        "# public score: ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/v3/ph9z3jvd3h32sv82kzzmklym0000gn/T/ipykernel_847/2076207791.py:35: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
            "  model = KerasClassifier(build_fn=create_model, verbose=0)\n",
            "2021-11-27 11:57:57.143838: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-27 11:57:57.144015: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-27 11:57:57.144160: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-27 11:57:57.148570: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-27 11:57:57.155404: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best: 0.742222 using {'batch_size': 2048, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.742222 (0.002128) with: {'batch_size': 2048, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "trian score: 0.7525711111111111\n",
            "test score: 0.7442333333333333\n",
            "Precision = 75.23% , recall = 76.18% and f1_score=75.70% of the % model on the training data.\n",
            "Precision = 74.43% , recall = 75.37% and f1_score=74.90% of the % model on the validation data.\n",
            "ROC_AUC Score = 75.25%  of the % model on the training data.\n",
            "ROC_AUC Score = 74.41%  of the % model on the validation data.\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "X_test = X_test.reset_index(drop=True)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "num_cols = X_train.select_dtypes(['integer', 'float']).columns\n",
        "\n",
        "X_train = pd.DataFrame(scaler.fit_transform(X_train[num_cols]), columns=num_cols)\n",
        "X_test = pd.DataFrame(scaler.fit_transform(X_test[num_cols]), columns=num_cols)\n",
        "\n",
        "# Function to create model, required for KerasClassifier\n",
        "def create_model(optimizer='rmsprop', init='glorot_uniform'):\n",
        "\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(128, input_dim=100, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(64, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(32, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\t#model.add(Dense(16, kernel_initializer=init, activation='relu'))\n",
        "\t#model.add(Dropout(0.2))\t\n",
        "\tmodel.add(Dense(8, kernel_initializer=init, activation='relu')) # new layer\n",
        "\tmodel.add(Dropout(0.2))\t\t\n",
        "\tmodel.add(Dense(1, kernel_initializer=init, activation='sigmoid'))\n",
        "\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "\treturn model\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "\n",
        "# grid search epochs, batch size and optimizer\n",
        "optimizers = ['adam']\n",
        "init = ['glorot_uniform']\n",
        "epochs = [100]\n",
        "batches = [2048]\n",
        "\n",
        "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "report_results('KerasClassifier', y_test, y_train, grid, grid_result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/v3/ph9z3jvd3h32sv82kzzmklym0000gn/T/ipykernel_847/3075640298.py:15: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
            "  model = KerasClassifier(build_fn=create_model, verbose=0)\n",
            "2021-11-27 13:02:10.295548: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-27 13:02:10.297096: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-27 13:02:10.297238: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-27 13:02:10.306111: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-27 13:02:10.306125: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "# re-train with best parameter\n",
        "\n",
        "X_train, y_train = X, y\n",
        "\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "X_test = X_test.reset_index(drop=True)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "num_cols = X_train.select_dtypes(['integer', 'float']).columns\n",
        "\n",
        "X_train = pd.DataFrame(scaler.fit_transform(X_train[num_cols]), columns=num_cols)\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "\n",
        "optimizers = ['adam']\n",
        "init = ['glorot_uniform']\n",
        "epochs = [100]\n",
        "batches = [2048]\n",
        "\n",
        "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# public score: 0.74561"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/v3/ph9z3jvd3h32sv82kzzmklym0000gn/T/ipykernel_847/3764264529.py:35: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
            "  model = KerasClassifier(build_fn=create_model, verbose=0)\n",
            "2021-11-27 16:08:29.596488: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-27 16:08:29.597327: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-27 16:08:29.605309: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-27 16:08:29.605840: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-27 16:08:29.610761: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best: 0.741776 using {'batch_size': 2048, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.741776 (0.001765) with: {'batch_size': 2048, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "trian score: 0.7526488888888889\n",
            "test score: 0.7445933333333333\n",
            "Precision = 75.21% , recall = 76.24% and f1_score=75.72% of the % model on the training data.\n",
            "Precision = 74.44% , recall = 75.47% and f1_score=74.95% of the % model on the validation data.\n",
            "ROC_AUC Score = 75.25%  of the % model on the training data.\n",
            "ROC_AUC Score = 74.45%  of the % model on the validation data.\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "X_test = X_test.reset_index(drop=True)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "num_cols = X_train.select_dtypes(['integer', 'float']).columns\n",
        "\n",
        "X_train = pd.DataFrame(scaler.fit_transform(X_train[num_cols]), columns=num_cols)\n",
        "X_test = pd.DataFrame(scaler.fit_transform(X_test[num_cols]), columns=num_cols)\n",
        "\n",
        "# Function to create model, required for KerasClassifier\n",
        "def create_model(optimizer='rmsprop', init='glorot_uniform'):\n",
        "\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(128, input_dim=100, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(64, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(32, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(16, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\t\n",
        "\t#model.add(Dense(8, kernel_initializer=init, activation='relu')) # new layer\n",
        "\t#model.add(Dropout(0.2))\t\t\n",
        "\tmodel.add(Dense(1, kernel_initializer=init, activation='sigmoid'))\n",
        "\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "\treturn model\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "\n",
        "# grid search epochs, batch size and optimizer\n",
        "optimizers = ['adam']\n",
        "init = ['glorot_uniform']\n",
        "epochs = [100]\n",
        "batches = [2048]\n",
        "\n",
        "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "report_results('KerasClassifier', y_test, y_train, grid, grid_result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/v3/ph9z3jvd3h32sv82kzzmklym0000gn/T/ipykernel_847/3426818550.py:15: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
            "  model = KerasClassifier(build_fn=create_model, verbose=0)\n",
            "2021-11-27 16:17:13.007588: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "# re-train with best parameter\n",
        "\n",
        "X_train, y_train = X, y\n",
        "\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "X_test = X_test.reset_index(drop=True)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "num_cols = X_train.select_dtypes(['integer', 'float']).columns\n",
        "\n",
        "X_train = pd.DataFrame(scaler.fit_transform(X_train[num_cols]), columns=num_cols)\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "\n",
        "optimizers = ['adam']\n",
        "init = ['glorot_uniform']\n",
        "epochs = [100]\n",
        "batches = [2048]\n",
        "\n",
        "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# public score: 0.74673"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<magic-timeit>:35: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
            "2021-11-27 17:20:28.129279: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-27 17:20:28.129290: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-27 17:20:28.129567: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-27 17:20:28.129568: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-27 17:20:28.130184: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best: 0.741091 using {'batch_size': 2048, 'epochs': 500, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.741091 (0.001894) with: {'batch_size': 2048, 'epochs': 500, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Found input variables with inconsistent numbers of samples: [450000, 600000]",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/var/folders/v3/ph9z3jvd3h32sv82kzzmklym0000gn/T/ipykernel_847/2761769969.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'timeit'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\\n\\nX_train = X_train.reset_index(drop=True)\\nX_test = X_test.reset_index(drop=True)\\n\\nscaler = StandardScaler()\\n\\nnum_cols = X_train.select_dtypes(['integer', 'float']).columns\\n\\nX_train = pd.DataFrame(scaler.fit_transform(X_train[num_cols]), columns=num_cols)\\nX_test = pd.DataFrame(scaler.fit_transform(X_test[num_cols]), columns=num_cols)\\n\\n# Function to create model, required for KerasClassifier\\ndef create_model(optimizer='rmsprop', init='glorot_uniform'):\\n\\n\\t# create model\\n\\tmodel = Sequential()\\n\\tmodel.add(Dense(128, input_dim=100, kernel_initializer=init, activation='relu'))\\n\\tmodel.add(Dropout(0.2))\\n\\tmodel.add(Dense(64, kernel_initializer=init, activation='relu'))\\n\\tmodel.add(Dropout(0.2))\\n\\tmodel.add(Dense(32, kernel_initializer=init, activation='relu'))\\n\\tmodel.add(Dropout(0.2))\\n\\tmodel.add(Dense(16, kernel_initializer=init, activation='relu'))\\n\\tmodel.add(Dropout(0.2))\\t\\n\\t#model.add(Dense(8, kernel_initializer=init, activation='relu')) # new layer\\n\\t#model.add(Dropout(0.2))\\t\\t\\n\\tmodel.add(Dense(1, kernel_initializer=init, activation='sigmoid'))\\n\\n\\t# Compile model\\n\\tmodel.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\\n\\treturn model\\n\\n# create model\\nmodel = KerasClassifier(build_fn=create_model, verbose=0)\\n\\n# grid search epochs, batch size and optimizer\\noptimizers = ['adam']\\ninit = ['glorot_uniform']\\nepochs = [500]\\nbatches = [2048]\\n\\nparam_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init)\\ngrid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=6)\\ngrid_result = grid.fit(X_train, y_train)\\n\\nreport_results('KerasClassifier', y_test, y_train, grid, grid_result)\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2404\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2405\u001b[0m                 \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2406\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2407\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/decorator.py\u001b[0m in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mkwsyntax\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcaller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextras\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[0mfun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1167\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                 \u001b[0mnumber\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                 \u001b[0mtime_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1170\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtime_number\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtimeit\u001b[0;34m(self, number)\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0mtiming\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgcold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<magic-timeit>\u001b[0m in \u001b[0;36minner\u001b[0;34m(_it, _timer)\u001b[0m\n",
            "\u001b[0;32m/var/folders/v3/ph9z3jvd3h32sv82kzzmklym0000gn/T/ipykernel_847/2795547525.py\u001b[0m in \u001b[0;36mreport_results\u001b[0;34m(model_name, y_test, y_train, grid_search_model, grid_search_results)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0my_hat_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mtrain_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'trian score: {train_score / y_train.shape[0]}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multilabel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \"\"\"\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    329\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    332\u001b[0m             \u001b[0;34m\"Found input variables with inconsistent numbers of samples: %r\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;34m%\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [450000, 600000]"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "X_test = X_test.reset_index(drop=True)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "num_cols = X_train.select_dtypes(['integer', 'float']).columns\n",
        "\n",
        "X_train = pd.DataFrame(scaler.fit_transform(X_train[num_cols]), columns=num_cols)\n",
        "X_test = pd.DataFrame(scaler.fit_transform(X_test[num_cols]), columns=num_cols)\n",
        "\n",
        "# Function to create model, required for KerasClassifier\n",
        "def create_model(optimizer='rmsprop', init='glorot_uniform'):\n",
        "\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(128, input_dim=100, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(64, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(32, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(16, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\t\n",
        "\t#model.add(Dense(8, kernel_initializer=init, activation='relu')) # new layer\n",
        "\t#model.add(Dropout(0.2))\t\t\n",
        "\tmodel.add(Dense(1, kernel_initializer=init, activation='sigmoid'))\n",
        "\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "\treturn model\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "\n",
        "# grid search epochs, batch size and optimizer\n",
        "optimizers = ['adam']\n",
        "init = ['glorot_uniform']\n",
        "epochs = [500]\n",
        "batches = [2048]\n",
        "\n",
        "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "report_results('KerasClassifier', y_test, y_train, grid, grid_result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best: 0.743600 using {'batch_size': 2048, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.743600 (0.004021) with: {'batch_size': 2048, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "trian score: 0.7515583333333333\n",
            "test score: 0.7525733333333333\n",
            "Precision = 75.17% , recall = 76.01% and f1_score=75.59% of the % model on the training data.\n",
            "Precision = 75.31% , recall = 76.08% and f1_score=75.69% of the % model on the validation data.\n",
            "ROC_AUC Score = 75.15%  of the % model on the training data.\n",
            "ROC_AUC Score = 75.25%  of the % model on the validation data.\n"
          ]
        }
      ],
      "source": [
        "report_results('KerasClassifier', y_test, y_train, grid, grid_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/v3/ph9z3jvd3h32sv82kzzmklym0000gn/T/ipykernel_847/3104537658.py:15: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
            "  model = KerasClassifier(build_fn=create_model, verbose=0)\n",
            "2021-11-27 18:56:07.153981: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-27 18:56:07.158065: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-27 18:56:07.160900: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-27 18:56:07.163514: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-27 18:56:07.169016: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "# re-train with best parameter\n",
        "\n",
        "X_train, y_train = X, y\n",
        "\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "X_test = X_test.reset_index(drop=True)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "num_cols = X_train.select_dtypes(['integer', 'float']).columns\n",
        "\n",
        "X_train = pd.DataFrame(scaler.fit_transform(X_train[num_cols]), columns=num_cols)\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "\n",
        "optimizers = ['adam']\n",
        "init = ['glorot_uniform']\n",
        "epochs = [500]\n",
        "batches = [2048]\n",
        "\n",
        "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# public score: 0.74629"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best: 0.743638 using {'batch_size': 2048, 'epochs': 500, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.743638 (0.004101) with: {'batch_size': 2048, 'epochs': 500, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "trian score: 0.7545483333333334\n",
            "test score: 0.7556666666666667\n",
            "Precision = 75.51% , recall = 76.21% and f1_score=75.86% of the % model on the training data.\n",
            "Precision = 75.65% , recall = 76.31% and f1_score=75.98% of the % model on the validation data.\n",
            "ROC_AUC Score = 75.45%  of the % model on the training data.\n",
            "ROC_AUC Score = 75.56%  of the % model on the validation data.\n"
          ]
        }
      ],
      "source": [
        "report_results('KerasClassifier', y_test, y_train, grid, grid_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/v3/ph9z3jvd3h32sv82kzzmklym0000gn/T/ipykernel_847/1513086491.py:35: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
            "  model = KerasClassifier(build_fn=create_model, verbose=0)\n",
            "2021-11-27 19:57:47.293661: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-27 19:57:47.293661: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-27 19:57:47.293884: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-27 19:57:47.294078: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-27 19:57:47.295039: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-27 19:57:47.295832: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-27 19:57:47.296851: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-27 19:57:47.298384: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best: 0.742302 using {'batch_size': 2048, 'epochs': 120, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.742136 (0.002099) with: {'batch_size': 2048, 'epochs': 80, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.742302 (0.002103) with: {'batch_size': 2048, 'epochs': 120, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "trian score: 0.7525044444444444\n",
            "test score: 0.74482\n",
            "Precision = 75.23% , recall = 76.15% and f1_score=75.69% of the % model on the training data.\n",
            "Precision = 74.51% , recall = 75.40% and f1_score=74.95% of the % model on the validation data.\n",
            "ROC_AUC Score = 75.24%  of the % model on the training data.\n",
            "ROC_AUC Score = 74.47%  of the % model on the validation data.\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "X_test = X_test.reset_index(drop=True)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "num_cols = X_train.select_dtypes(['integer', 'float']).columns\n",
        "\n",
        "X_train = pd.DataFrame(scaler.fit_transform(X_train[num_cols]), columns=num_cols)\n",
        "X_test = pd.DataFrame(scaler.fit_transform(X_test[num_cols]), columns=num_cols)\n",
        "\n",
        "# Function to create model, required for KerasClassifier\n",
        "def create_model(optimizer='rmsprop', init='glorot_uniform'):\n",
        "\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(128, input_dim=100, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(64, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(32, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(16, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\t\n",
        "\t#model.add(Dense(8, kernel_initializer=init, activation='relu')) # new layer\n",
        "\t#model.add(Dropout(0.2))\t\t\n",
        "\tmodel.add(Dense(1, kernel_initializer=init, activation='sigmoid'))\n",
        "\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "\treturn model\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "\n",
        "# grid search epochs, batch size and optimizer\n",
        "optimizers = ['adam']\n",
        "init = ['glorot_uniform']\n",
        "epochs = [80, 120]\n",
        "batches = [2048]\n",
        "\n",
        "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "report_results('KerasClassifier', y_test, y_train, grid, grid_result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/v3/ph9z3jvd3h32sv82kzzmklym0000gn/T/ipykernel_847/1853612221.py:15: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
            "  model = KerasClassifier(build_fn=create_model, verbose=0)\n",
            "2021-11-27 21:35:37.736278: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-27 21:35:37.736553: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-27 21:35:37.736580: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-27 21:35:37.736602: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-27 21:35:37.737383: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "# re-train with best parameter\n",
        "\n",
        "X_train, y_train = X, y\n",
        "\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "X_test = X_test.reset_index(drop=True)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "num_cols = X_train.select_dtypes(['integer', 'float']).columns\n",
        "\n",
        "X_train = pd.DataFrame(scaler.fit_transform(X_train[num_cols]), columns=num_cols)\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "\n",
        "optimizers = ['adam']\n",
        "init = ['glorot_uniform']\n",
        "epochs = [120]\n",
        "batches = [2048]\n",
        "\n",
        "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# public score: 0.74629"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best: 0.743790 using {'batch_size': 2048, 'epochs': 120, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.743790 (0.004240) with: {'batch_size': 2048, 'epochs': 120, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "trian score: 0.7513783333333334\n",
            "test score: 0.7522133333333333\n",
            "Precision = 75.04% , recall = 76.22% and f1_score=75.62% of the % model on the training data.\n",
            "Precision = 75.16% , recall = 76.28% and f1_score=75.71% of the % model on the validation data.\n",
            "ROC_AUC Score = 75.12%  of the % model on the training data.\n",
            "ROC_AUC Score = 75.21%  of the % model on the validation data.\n"
          ]
        }
      ],
      "source": [
        "report_results('KerasClassifier', y_test, y_train, grid, grid_result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/v3/ph9z3jvd3h32sv82kzzmklym0000gn/T/ipykernel_847/3424149821.py:37: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
            "  model = KerasClassifier(build_fn=create_model, verbose=0)\n",
            "2021-11-28 00:46:57.108184: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-28 00:46:57.108775: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-28 00:46:57.109613: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-28 00:46:57.111033: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-28 00:46:57.115986: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best: 0.742293 using {'batch_size': 2048, 'epochs': 1000, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.742293 (0.001798) with: {'batch_size': 2048, 'epochs': 1000, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "trian score: 0.7566511111111112\n",
            "test score: 0.7454266666666667\n",
            "Precision = 75.66% , recall = 76.51% and f1_score=76.08% of the % model on the training data.\n",
            "Precision = 74.61% , recall = 75.37% and f1_score=74.99% of the % model on the validation data.\n",
            "ROC_AUC Score = 75.66%  of the % model on the training data.\n",
            "ROC_AUC Score = 74.53%  of the % model on the validation data.\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "X_test = X_test.reset_index(drop=True)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "num_cols = X_train.select_dtypes(['integer', 'float']).columns\n",
        "\n",
        "X_train = pd.DataFrame(scaler.fit_transform(X_train[num_cols]), columns=num_cols)\n",
        "X_test = pd.DataFrame(scaler.fit_transform(X_test[num_cols]), columns=num_cols)\n",
        "\n",
        "# Function to create model, required for KerasClassifier\n",
        "def create_model(optimizer='rmsprop', init='glorot_uniform'):\n",
        "\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(128, input_dim=100, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(64, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(32, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(16, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\t\n",
        "\tmodel.add(Dense(16, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\t\n",
        "\t#model.add(Dense(8, kernel_initializer=init, activation='relu'))\n",
        "\t#model.add(Dropout(0.2))\t\t\n",
        "\tmodel.add(Dense(1, kernel_initializer=init, activation='sigmoid'))\n",
        "\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.0001), metrics=['accuracy'])\n",
        "\treturn model\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "\n",
        "# grid search epochs, batch size and optimizer\n",
        "optimizers = ['adam']\n",
        "init = ['glorot_uniform']\n",
        "epochs = [1000]\n",
        "batches = [2048]\n",
        "\n",
        "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "report_results('KerasClassifier', y_test, y_train, grid, grid_result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tune Batch Size and Number of Eposchs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/v3/ph9z3jvd3h32sv82kzzmklym0000gn/T/ipykernel_847/4096780091.py:2: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
            "  model = KerasClassifier(build_fn=create_keras_sequential_model, verbose=0)\n",
            "2021-11-26 16:19:31.204977: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 16:19:31.205013: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 16:19:31.205020: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 16:19:31.205074: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 16:19:31.205131: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 16:19:31.205158: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 16:19:31.205342: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 16:19:31.206394: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/Users/kyle/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:702: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n",
            "2021-11-26 16:47:46.147060: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 16:47:48.785714: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 16:47:56.184710: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 16:48:02.622377: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 16:48:04.925174: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 16:48:59.401037: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 16:49:04.971432: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 16:56:14.352325: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 17:33:16.229410: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 17:39:31.423505: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 17:44:30.233627: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 17:44:33.851318: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 17:44:57.369055: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 17:45:01.148339: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 17:46:25.400191: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 17:47:52.903255: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 17:52:33.354252: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 18:05:19.387584: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 18:05:45.593657: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 18:07:30.613071: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 18:11:01.027648: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 18:16:19.500629: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 18:18:43.166753: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 18:20:04.466342: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 18:21:10.834343: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 18:57:15.489856: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "# create model\n",
        "model = KerasClassifier(build_fn=create_keras_sequential_model, verbose=0)\n",
        "\n",
        "# grid search epochs, batch size and optimizer\n",
        "#optimizers = ['rmsprop', 'adam']\n",
        "#init = ['glorot_uniform', 'normal', 'uniform']\n",
        "#epochs = [100, 150]\n",
        "#batches = [1024, 2048]\n",
        "\n",
        "optimizers = ['adam']\n",
        "init = ['glorot_uniform', 'normal', 'uniform']\n",
        "epochs = [100, 200, 300]\n",
        "batches = [1024, 2048]\n",
        "\n",
        "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best: 0.742073 using {'batch_size': 2048, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.741736 (0.002229) with: {'batch_size': 1024, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.740542 (0.002268) with: {'batch_size': 1024, 'epochs': 100, 'init': 'normal', 'optimizer': 'adam'}\n",
            "0.740862 (0.002184) with: {'batch_size': 1024, 'epochs': 100, 'init': 'uniform', 'optimizer': 'adam'}\n",
            "0.741229 (0.002358) with: {'batch_size': 1024, 'epochs': 200, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.740569 (0.002564) with: {'batch_size': 1024, 'epochs': 200, 'init': 'normal', 'optimizer': 'adam'}\n",
            "0.740144 (0.001736) with: {'batch_size': 1024, 'epochs': 200, 'init': 'uniform', 'optimizer': 'adam'}\n",
            "0.740784 (0.002146) with: {'batch_size': 1024, 'epochs': 300, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.740707 (0.001958) with: {'batch_size': 1024, 'epochs': 300, 'init': 'normal', 'optimizer': 'adam'}\n",
            "0.740747 (0.001736) with: {'batch_size': 1024, 'epochs': 300, 'init': 'uniform', 'optimizer': 'adam'}\n",
            "0.742073 (0.002204) with: {'batch_size': 2048, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.740927 (0.002121) with: {'batch_size': 2048, 'epochs': 100, 'init': 'normal', 'optimizer': 'adam'}\n",
            "0.741124 (0.001895) with: {'batch_size': 2048, 'epochs': 100, 'init': 'uniform', 'optimizer': 'adam'}\n",
            "0.741716 (0.002304) with: {'batch_size': 2048, 'epochs': 200, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.741069 (0.002017) with: {'batch_size': 2048, 'epochs': 200, 'init': 'normal', 'optimizer': 'adam'}\n",
            "0.741260 (0.001941) with: {'batch_size': 2048, 'epochs': 200, 'init': 'uniform', 'optimizer': 'adam'}\n",
            "0.741604 (0.002134) with: {'batch_size': 2048, 'epochs': 300, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.740611 (0.002372) with: {'batch_size': 2048, 'epochs': 300, 'init': 'normal', 'optimizer': 'adam'}\n",
            "0.740669 (0.002684) with: {'batch_size': 2048, 'epochs': 300, 'init': 'uniform', 'optimizer': 'adam'}\n",
            "trian score: 0.7528977777777778\n",
            "test score: 0.7439533333333334\n",
            "Precision = 75.15% , recall = 76.43% and f1_score=75.79% of the % model on the training data.\n",
            "Precision = 74.34% , recall = 75.50% and f1_score=74.91% of the % model on the validation data.\n",
            "ROC_AUC Score = 75.28%  of the % model on the training data.\n",
            "ROC_AUC Score = 74.38%  of the % model on the validation data.\n"
          ]
        }
      ],
      "source": [
        "report_results('KerasClassifier', y_test, y_train, grid, grid_result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/v3/ph9z3jvd3h32sv82kzzmklym0000gn/T/ipykernel_847/219623009.py:15: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
            "  model = KerasClassifier(build_fn=create_keras_sequential_model, verbose=0)\n",
            "2021-11-26 19:07:34.831019: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 19:07:34.831102: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 19:07:34.833789: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 19:07:34.835071: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 19:07:34.838053: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "# re-train with best parameter and full set of train dataset\n",
        "\n",
        "X_train, y_train = X, y\n",
        "\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "X_test = X_test.reset_index(drop=True)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "num_cols = X_train.select_dtypes(['integer', 'float']).columns\n",
        "\n",
        "X_train = pd.DataFrame(scaler.fit_transform(X_train[num_cols]), columns=num_cols)\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_keras_sequential_model, verbose=0)\n",
        "\n",
        "optimizers = ['adam']\n",
        "init = ['glorot_uniform']\n",
        "epochs = [100]\n",
        "batches = [2048]\n",
        "\n",
        "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# public score: 0.74591 (second run)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best: 0.743600 using {'batch_size': 2048, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.743600 (0.004309) with: {'batch_size': 2048, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "trian score: 0.7510366666666667\n",
            "test score: 0.75208\n",
            "Precision = 75.08% , recall = 76.03% and f1_score=75.55% of the % model on the training data.\n",
            "Precision = 75.22% , recall = 76.11% and f1_score=75.66% of the % model on the validation data.\n",
            "ROC_AUC Score = 75.09%  of the % model on the training data.\n",
            "ROC_AUC Score = 75.20%  of the % model on the validation data.\n"
          ]
        }
      ],
      "source": [
        "report_results('KerasClassifier', y_test, y_train, grid, grid_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Round 2 - Number of Eposchs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/v3/ph9z3jvd3h32sv82kzzmklym0000gn/T/ipykernel_847/2622259205.py:2: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
            "  model = KerasClassifier(build_fn=create_keras_sequential_model, verbose=0)\n",
            "2021-11-26 19:20:43.306463: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 19:20:43.315967: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 19:20:43.316200: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 19:20:43.319287: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 19:20:43.320722: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best: 0.742638 using {'batch_size': 2048, 'epochs': 1000, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.742638 (0.004250) with: {'batch_size': 2048, 'epochs': 1000, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "trian score: 0.75463\n",
            "test score: 0.7556133333333334\n",
            "Precision = 75.39% , recall = 76.47% and f1_score=75.93% of the % model on the training data.\n",
            "Precision = 75.52% , recall = 76.54% and f1_score=76.03% of the % model on the validation data.\n",
            "ROC_AUC Score = 75.45%  of the % model on the training data.\n",
            "ROC_AUC Score = 75.55%  of the % model on the validation data.\n"
          ]
        }
      ],
      "source": [
        "# create model\n",
        "model = KerasClassifier(build_fn=create_keras_sequential_model, verbose=0)\n",
        "\n",
        "# grid search epochs, batch size and optimizer\n",
        "optimizers = ['adam']\n",
        "init = ['glorot_uniform']\n",
        "epochs = [1000]\n",
        "batches = [2048]\n",
        "\n",
        "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "report_results('KerasClassifier', y_test, y_train, grid, grid_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/v3/ph9z3jvd3h32sv82kzzmklym0000gn/T/ipykernel_847/3139461891.py:15: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
            "  model = KerasClassifier(build_fn=create_keras_sequential_model, verbose=0)\n",
            "2021-11-26 20:56:13.537228: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 20:56:13.539045: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 20:56:13.539699: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 20:56:13.544315: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 20:56:13.548724: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "# re-train with best parameter and full set of train dataset\n",
        "\n",
        "X_train, y_train = X, y\n",
        "\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "X_test = X_test.reset_index(drop=True)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "num_cols = X_train.select_dtypes(['integer', 'float']).columns\n",
        "\n",
        "X_train = pd.DataFrame(scaler.fit_transform(X_train[num_cols]), columns=num_cols)\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_keras_sequential_model, verbose=0)\n",
        "\n",
        "optimizers = ['adam']\n",
        "init = ['glorot_uniform']\n",
        "epochs = [1000]\n",
        "batches = [2048]\n",
        "\n",
        "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# public score: ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best: 0.743443 using {'batch_size': 2048, 'epochs': 1000, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.743443 (0.004062) with: {'batch_size': 2048, 'epochs': 1000, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "trian score: 0.75469\n",
            "test score: 0.7556133333333334\n",
            "Precision = 75.45% , recall = 76.37% and f1_score=75.91% of the % model on the training data.\n",
            "Precision = 75.56% , recall = 76.47% and f1_score=76.01% of the % model on the validation data.\n",
            "ROC_AUC Score = 75.46%  of the % model on the training data.\n",
            "ROC_AUC Score = 75.55%  of the % model on the validation data.\n"
          ]
        }
      ],
      "source": [
        "report_results('KerasClassifier', y_test, y_train, grid, grid_result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tune the Training Optimization Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/v3/ph9z3jvd3h32sv82kzzmklym0000gn/T/ipykernel_76605/467874160.py:2: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
            "  model = KerasClassifier(build_fn=create_keras_sequential_model, verbose=0)\n",
            "2021-11-25 23:04:18.145394: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-25 23:04:18.248205: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "# create model\n",
        "model = KerasClassifier(build_fn=create_keras_sequential_model, verbose=0)\n",
        "\n",
        "#optimizers = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
        "optimizers = ['Adamax']\n",
        "init = ['glorot_uniform']\n",
        "epochs = [100]\n",
        "batches = [2048]\n",
        "\n",
        "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best: 0.744527 using {'batch_size': 2048, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'Adamax'}\n",
            "0.744527 (0.003838) with: {'batch_size': 2048, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'Adamax'}\n",
            "trian score: 0.7519483333333333\n",
            "test score: 0.7527933333333333\n",
            "Precision = 75.20% , recall = 76.06% and f1_score=75.63% of the % model on the training data.\n",
            "Precision = 75.32% , recall = 76.12% and f1_score=75.72% of the % model on the validation data.\n",
            "ROC_AUC Score = 75.18%  of the % model on the training data.\n",
            "ROC_AUC Score = 75.27%  of the % model on the validation data.\n"
          ]
        }
      ],
      "source": [
        "report_results('KerasClassifier', y_test, y_hat_test, y_train, y_hat_train, grid, grid_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/v3/ph9z3jvd3h32sv82kzzmklym0000gn/T/ipykernel_76605/3837577376.py:15: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
            "  model = KerasClassifier(build_fn=create_keras_sequential_model, verbose=0)\n",
            "2021-11-25 23:19:55.142051: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-25 23:19:55.142488: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-25 23:19:55.153509: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-25 23:19:55.154932: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-25 23:19:55.156289: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "# re-train with best parameter\n",
        "\n",
        "X_train, y_train = X, y\n",
        "\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "X_test = X_test.reset_index(drop=True)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "num_cols = X_train.select_dtypes(['integer', 'float']).columns\n",
        "\n",
        "X_train = pd.DataFrame(scaler.fit_transform(X_train[num_cols]), columns=num_cols)\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_keras_sequential_model, verbose=0)\n",
        "\n",
        "optimizers = ['Adamax']\n",
        "init = ['glorot_uniform']\n",
        "epochs = [100]\n",
        "batches = [2048]\n",
        "\n",
        "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# public score: 0.74599"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tune Network Weight Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/v3/ph9z3jvd3h32sv82kzzmklym0000gn/T/ipykernel_76605/2345569135.py:2: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
            "  model = KerasClassifier(build_fn=create_keras_sequential_model, verbose=0)\n",
            "/Users/kyle/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:702: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n",
            "2021-11-26 00:23:42.312301: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 00:23:42.316979: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 00:23:42.319685: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 00:23:42.840614: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 00:23:46.541463: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 00:24:09.357765: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 00:24:25.677908: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 00:25:07.316533: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 00:53:46.803624: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "# create model\n",
        "model = KerasClassifier(build_fn=create_keras_sequential_model, verbose=0)\n",
        "\n",
        "#optimizers = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
        "optimizers = ['Adamax']\n",
        "init = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
        "epochs = [100]\n",
        "batches = [2048]\n",
        "learn_rate = [0.001, 0.01]\n",
        "momentum = [0.0, 0.2, 0.4]\n",
        "\n",
        "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "#grid_result = grid.fit(X_train, y_train)\n",
        "grid_result = grid.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best: 0.744352 using {'batch_size': 2048, 'epochs': 100, 'init': 'he_uniform', 'optimizer': 'Adamax'}\n",
            "0.742848 (0.004295) with: {'batch_size': 2048, 'epochs': 100, 'init': 'uniform', 'optimizer': 'Adamax'}\n",
            "0.743383 (0.003994) with: {'batch_size': 2048, 'epochs': 100, 'init': 'lecun_uniform', 'optimizer': 'Adamax'}\n",
            "0.743197 (0.004248) with: {'batch_size': 2048, 'epochs': 100, 'init': 'normal', 'optimizer': 'Adamax'}\n",
            "0.460970 (0.027027) with: {'batch_size': 2048, 'epochs': 100, 'init': 'zero', 'optimizer': 'Adamax'}\n",
            "0.743533 (0.004100) with: {'batch_size': 2048, 'epochs': 100, 'init': 'glorot_normal', 'optimizer': 'Adamax'}\n",
            "0.743728 (0.004226) with: {'batch_size': 2048, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'Adamax'}\n",
            "0.744147 (0.004041) with: {'batch_size': 2048, 'epochs': 100, 'init': 'he_normal', 'optimizer': 'Adamax'}\n",
            "0.744352 (0.004369) with: {'batch_size': 2048, 'epochs': 100, 'init': 'he_uniform', 'optimizer': 'Adamax'}\n",
            "trian score: 0.75103\n",
            "test score: 0.75176\n",
            "Precision = 75.03% , recall = 76.13% and f1_score=75.58% of the % model on the training data.\n",
            "Precision = 75.13% , recall = 76.20% and f1_score=75.66% of the % model on the validation data.\n",
            "ROC_AUC Score = 75.09%  of the % model on the training data.\n",
            "ROC_AUC Score = 75.16%  of the % model on the validation data.\n"
          ]
        }
      ],
      "source": [
        "report_results('KerasClassifier', y_test, y_hat_test, y_train, y_hat_train, grid, grid_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/v3/ph9z3jvd3h32sv82kzzmklym0000gn/T/ipykernel_76605/184325068.py:15: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
            "  model = KerasClassifier(build_fn=create_keras_sequential_model, verbose=0)\n",
            "2021-11-26 07:46:38.863666: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 07:46:38.863796: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 07:46:38.863824: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 07:46:38.864180: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 07:46:38.865912: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "# re-train with best parameter\n",
        "\n",
        "X_train, y_train = X, y\n",
        "\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "X_test = X_test.reset_index(drop=True)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "num_cols = X_train.select_dtypes(['integer', 'float']).columns\n",
        "\n",
        "X_train = pd.DataFrame(scaler.fit_transform(X_train[num_cols]), columns=num_cols)\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_keras_sequential_model, verbose=0)\n",
        "\n",
        "optimizers = ['Adamax']\n",
        "init = ['he_uniform']\n",
        "epochs = [100]\n",
        "batches = [2048]\n",
        "\n",
        "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# public score: 0.74599"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tune Learning Rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/v3/ph9z3jvd3h32sv82kzzmklym0000gn/T/ipykernel_76605/3938506379.py:19: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
            "  model = KerasClassifier(build_fn=create_learning_rate_model, verbose=0)\n",
            "2021-11-26 08:22:11.361209: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 08:22:11.361474: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 08:22:11.363451: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 08:22:11.364469: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 08:22:11.364604: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 08:22:11.364612: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 08:22:11.364604: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 08:22:11.375605: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best: 0.744832 using {'batch_size': 2048, 'epochs': 100, 'init': 'glorot_uniform', 'learn_rate': 0.0001, 'optimizer': 'adam'}\n",
            "0.744832 (0.004127) with: {'batch_size': 2048, 'epochs': 100, 'init': 'glorot_uniform', 'learn_rate': 0.0001, 'optimizer': 'adam'}\n",
            "0.743538 (0.003970) with: {'batch_size': 2048, 'epochs': 100, 'init': 'glorot_uniform', 'learn_rate': 0.001, 'optimizer': 'adam'}\n",
            "0.732608 (0.004847) with: {'batch_size': 2048, 'epochs': 100, 'init': 'glorot_uniform', 'learn_rate': 0.01, 'optimizer': 'adam'}\n",
            "trian score: 0.7494466666666667\n",
            "test score: 0.7503733333333333\n",
            "Precision = 74.93% , recall = 75.88% and f1_score=75.40% of the % model on the training data.\n",
            "Precision = 75.06% , recall = 75.93% and f1_score=75.49% of the % model on the validation data.\n",
            "ROC_AUC Score = 74.93%  of the % model on the training data.\n",
            "ROC_AUC Score = 75.03%  of the % model on the validation data.\n"
          ]
        }
      ],
      "source": [
        "def create_learning_rate_model(optimizer='adam', init='glorot_uniform', learn_rate=0.001):\n",
        "\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(128, input_dim=100, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(64, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(32, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(16, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\t\n",
        "\tmodel.add(Dense(1, kernel_initializer=init, activation='sigmoid'))\n",
        "\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=learn_rate), metrics=['accuracy'])\n",
        "\n",
        "\treturn model\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_learning_rate_model, verbose=0)\n",
        "\n",
        "# grid search epochs, batch size and optimizer\n",
        "optimizers = ['adam']\n",
        "init = ['glorot_uniform']\n",
        "epochs = [100]\n",
        "batches = [2048]\n",
        "# Learning rate controls how much to update the weight at the end of each batch \n",
        "learn_rate = [0.0001, 0.001, 0.01]\n",
        "# and the momentum controls how much to let the previous update influence the current weight update\n",
        "momentum = [0.0, 0.2, 0.4] # ingnore for now\n",
        "\n",
        "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init, learn_rate=learn_rate)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "report_results('KerasClassifier', y_test, y_hat_test, y_train, y_hat_train, grid, grid_result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/v3/ph9z3jvd3h32sv82kzzmklym0000gn/T/ipykernel_76605/3824939399.py:32: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
            "  model = KerasClassifier(build_fn=create_keras_sequential_model, verbose=0)\n",
            "2021-11-26 09:16:22.577960: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 09:16:22.578015: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 09:16:22.578014: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 09:16:22.581560: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 09:16:22.583430: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "# re-train with best parameter\n",
        "\n",
        "def create_learning_rate_model(optimizer='adam', init='glorot_uniform', learn_rate=0.0001):\n",
        "\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(128, input_dim=100, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(64, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(32, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(16, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\t\n",
        "\tmodel.add(Dense(1, kernel_initializer=init, activation='sigmoid'))\n",
        "\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=learn_rate), metrics=['accuracy'])\n",
        "\n",
        "\treturn model\n",
        "\n",
        "X_train, y_train = X, y\n",
        "\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "X_test = X_test.reset_index(drop=True)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "num_cols = X_train.select_dtypes(['integer', 'float']).columns\n",
        "\n",
        "X_train = pd.DataFrame(scaler.fit_transform(X_train[num_cols]), columns=num_cols)\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_keras_sequential_model, verbose=0)\n",
        "\n",
        "optimizers = ['adam']\n",
        "init = ['glorot_uniform']\n",
        "epochs = [100]\n",
        "batches = [2048]\n",
        "\n",
        "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# public score: 0.74591"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_test_norm = pd.DataFrame(scaler.transform(test_data[num_cols]), columns = num_cols)\n",
        "\n",
        "test_predict = grid.predict_proba(data_test_norm)[::,1]\n",
        "test_predict = test_predict.astype(float)\n",
        "array = np.array(test_predict).tolist()\n",
        "df = pd.DataFrame(test_data['id'])\n",
        "df['id'] = df['id'].astype(int)\n",
        "df['target'] = np.array(array)\n",
        "df.to_csv('results/keras_results.csv', sep=',', encoding='utf-8', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/kyle/Documents/github-data-research-team/kaggle-competitions/tabular-playground-series-nov-2021/results\n",
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /Users/kyle/.kaggle/kaggle.json'\n",
            "100%|███████████████████████████████████████| 13.6M/13.6M [00:23<00:00, 596kB/s]\n",
            "Successfully submitted to Tabular Playground Series - Nov 2021/Users/kyle/Documents/github-data-research-team/kaggle-competitions/tabular-playground-series-nov-2021\n"
          ]
        }
      ],
      "source": [
        "%cd results/\n",
        "\n",
        "!kaggle  competitions  submit -c tabular-playground-series-nov-2021 -f keras_results.csv -m \"keras implementation\"\n",
        "\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /Users/kyle/.kaggle/kaggle.json'\n",
            "fileName           date                 description                         status    publicScore  privateScore  \n",
            "-----------------  -------------------  ----------------------------------  --------  -----------  ------------  \n",
            "keras_results.csv  2021-11-28 05:52:28  keras implementation                complete  0.74629      None          \n",
            "keras_results.csv  2021-11-28 03:51:50  keras implementation                complete  0.74629      None          \n",
            "keras_results.csv  2021-11-28 01:16:16  keras implementation                complete  0.74673      None          \n",
            "keras_results.csv  2021-11-27 21:11:01  keras implementation                complete  0.74561      None          \n",
            "keras_results.csv  2021-11-27 20:57:47  keras implementation                complete  0.74457      None          \n",
            "keras_results.csv  2021-11-27 06:36:55  keras implementation                complete  0.74616      None          \n",
            "keras_results.csv  2021-11-27 00:15:06  keras implementation                complete  0.74591      None          \n",
            "keras_results.csv  2021-11-26 06:57:18  keras implementation                complete  0.74599      None          \n",
            "keras_results.csv  2021-11-26 05:34:15  keras implementation                complete  0.74736      None          \n",
            "keras_results.csv  2021-11-26 05:08:25  keras implementation                complete  0.73143      None          \n",
            "keras_results.csv  2021-11-26 02:27:10  KerasClassfier baseline submission  complete  0.73143      None          \n",
            "keras_results.csv  2021-11-26 00:05:33  KerasClassfier baseline submission  complete  0.73289      None          \n",
            "keras_results.csv  2021-11-25 22:02:00  KerasClassfier baseline submission  complete  0.70475      None          \n",
            "keras_results.csv  2021-11-25 18:39:49  KerasClassfier baseline submission  complete  0.50284      None          \n",
            "keras_results.csv  2021-11-25 18:00:42  KerasClassfier baseline submission  complete  0.50297      None          \n"
          ]
        }
      ],
      "source": [
        "!kaggle competitions submissions -c tabular-playground-series-nov-2021 -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "keras_playground_nov_2021.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
