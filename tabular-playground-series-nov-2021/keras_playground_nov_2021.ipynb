{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "v7L8ZT2almwl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib as mplt\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_curve, roc_auc_score, f1_score\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import Adamax\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "lbqW8TTGp4hP"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NBgD1CYTqTsR"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv('Data/train.csv')\n",
        "test_data = pd.read_csv('Data/test.csv')\n",
        "submission_file = pd.read_csv('Data/sample_submission.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "uQl7kDa8qvxK"
      },
      "outputs": [],
      "source": [
        "train_data.drop(columns=['id'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlErXZC-qx2w",
        "outputId": "a6a3b783-a4b2-4d56-a2ae-d5933c79fdce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(600000, 101)\n",
            "(540000, 101)\n"
          ]
        }
      ],
      "source": [
        "print(train_data.shape)\n",
        "print(test_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FKLnnrkmq0OG"
      },
      "outputs": [],
      "source": [
        "X, y = train_data.drop(columns = ['target']), train_data['target']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ilm-eJVq3Sz",
        "outputId": "dc1c720b-d5a9-49e5-dbf3-1fa190ddf9a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dtypes = train_data.dtypes\n",
        "dtypes = dtypes[dtypes != 'object']\n",
        "features = list(set(dtypes.index) - set(['target']))\n",
        "\n",
        "len(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "YTGSonwyq6fu",
        "outputId": "87363f93-8c1f-492e-a2c4-225c4596344f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f0</th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "      <th>f6</th>\n",
              "      <th>f7</th>\n",
              "      <th>f8</th>\n",
              "      <th>f9</th>\n",
              "      <th>...</th>\n",
              "      <th>f91</th>\n",
              "      <th>f92</th>\n",
              "      <th>f93</th>\n",
              "      <th>f94</th>\n",
              "      <th>f95</th>\n",
              "      <th>f96</th>\n",
              "      <th>f97</th>\n",
              "      <th>f98</th>\n",
              "      <th>f99</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.106643</td>\n",
              "      <td>3.59437</td>\n",
              "      <td>132.8040</td>\n",
              "      <td>3.18428</td>\n",
              "      <td>0.081971</td>\n",
              "      <td>1.18859</td>\n",
              "      <td>3.73238</td>\n",
              "      <td>2.266270</td>\n",
              "      <td>2.09959</td>\n",
              "      <td>0.012330</td>\n",
              "      <td>...</td>\n",
              "      <td>1.09862</td>\n",
              "      <td>0.013331</td>\n",
              "      <td>-0.011715</td>\n",
              "      <td>0.052759</td>\n",
              "      <td>0.065400</td>\n",
              "      <td>4.211250</td>\n",
              "      <td>1.97877</td>\n",
              "      <td>0.085974</td>\n",
              "      <td>0.240496</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.125021</td>\n",
              "      <td>1.67336</td>\n",
              "      <td>76.5336</td>\n",
              "      <td>3.37825</td>\n",
              "      <td>0.099400</td>\n",
              "      <td>5.09366</td>\n",
              "      <td>1.27562</td>\n",
              "      <td>-0.471318</td>\n",
              "      <td>4.54594</td>\n",
              "      <td>0.037706</td>\n",
              "      <td>...</td>\n",
              "      <td>3.46017</td>\n",
              "      <td>0.017054</td>\n",
              "      <td>0.124863</td>\n",
              "      <td>0.154064</td>\n",
              "      <td>0.606848</td>\n",
              "      <td>-0.267928</td>\n",
              "      <td>2.57786</td>\n",
              "      <td>-0.020877</td>\n",
              "      <td>0.024719</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.036330</td>\n",
              "      <td>1.49747</td>\n",
              "      <td>233.5460</td>\n",
              "      <td>2.19435</td>\n",
              "      <td>0.026914</td>\n",
              "      <td>3.12694</td>\n",
              "      <td>5.05687</td>\n",
              "      <td>3.849460</td>\n",
              "      <td>1.80187</td>\n",
              "      <td>0.056995</td>\n",
              "      <td>...</td>\n",
              "      <td>4.88300</td>\n",
              "      <td>0.085222</td>\n",
              "      <td>0.032396</td>\n",
              "      <td>0.116092</td>\n",
              "      <td>-0.001688</td>\n",
              "      <td>-0.520069</td>\n",
              "      <td>2.14112</td>\n",
              "      <td>0.124464</td>\n",
              "      <td>0.148209</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.014077</td>\n",
              "      <td>0.24600</td>\n",
              "      <td>779.9670</td>\n",
              "      <td>1.89064</td>\n",
              "      <td>0.006948</td>\n",
              "      <td>1.53112</td>\n",
              "      <td>2.69800</td>\n",
              "      <td>4.517330</td>\n",
              "      <td>4.50332</td>\n",
              "      <td>0.123494</td>\n",
              "      <td>...</td>\n",
              "      <td>3.47439</td>\n",
              "      <td>-0.017103</td>\n",
              "      <td>-0.008100</td>\n",
              "      <td>0.062013</td>\n",
              "      <td>0.041193</td>\n",
              "      <td>0.511657</td>\n",
              "      <td>1.96860</td>\n",
              "      <td>0.040017</td>\n",
              "      <td>0.044873</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.003259</td>\n",
              "      <td>3.71542</td>\n",
              "      <td>156.1280</td>\n",
              "      <td>2.14772</td>\n",
              "      <td>0.018284</td>\n",
              "      <td>2.09859</td>\n",
              "      <td>4.15492</td>\n",
              "      <td>-0.038236</td>\n",
              "      <td>3.37145</td>\n",
              "      <td>0.034166</td>\n",
              "      <td>...</td>\n",
              "      <td>1.91059</td>\n",
              "      <td>-0.042943</td>\n",
              "      <td>0.105616</td>\n",
              "      <td>0.125072</td>\n",
              "      <td>0.037509</td>\n",
              "      <td>1.043790</td>\n",
              "      <td>1.07481</td>\n",
              "      <td>-0.012819</td>\n",
              "      <td>0.072798</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 101 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         f0       f1        f2       f3        f4       f5       f6        f7  \\\n",
              "0  0.106643  3.59437  132.8040  3.18428  0.081971  1.18859  3.73238  2.266270   \n",
              "1  0.125021  1.67336   76.5336  3.37825  0.099400  5.09366  1.27562 -0.471318   \n",
              "2  0.036330  1.49747  233.5460  2.19435  0.026914  3.12694  5.05687  3.849460   \n",
              "3 -0.014077  0.24600  779.9670  1.89064  0.006948  1.53112  2.69800  4.517330   \n",
              "4 -0.003259  3.71542  156.1280  2.14772  0.018284  2.09859  4.15492 -0.038236   \n",
              "\n",
              "        f8        f9  ...      f91       f92       f93       f94       f95  \\\n",
              "0  2.09959  0.012330  ...  1.09862  0.013331 -0.011715  0.052759  0.065400   \n",
              "1  4.54594  0.037706  ...  3.46017  0.017054  0.124863  0.154064  0.606848   \n",
              "2  1.80187  0.056995  ...  4.88300  0.085222  0.032396  0.116092 -0.001688   \n",
              "3  4.50332  0.123494  ...  3.47439 -0.017103 -0.008100  0.062013  0.041193   \n",
              "4  3.37145  0.034166  ...  1.91059 -0.042943  0.105616  0.125072  0.037509   \n",
              "\n",
              "        f96      f97       f98       f99  target  \n",
              "0  4.211250  1.97877  0.085974  0.240496       0  \n",
              "1 -0.267928  2.57786 -0.020877  0.024719       0  \n",
              "2 -0.520069  2.14112  0.124464  0.148209       0  \n",
              "3  0.511657  1.96860  0.040017  0.044873       0  \n",
              "4  1.043790  1.07481 -0.012819  0.072798       1  \n",
              "\n",
              "[5 rows x 101 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "EnnOgS9Rrfol",
        "outputId": "c4da73b4-1a40-4732-aacb-7936c1dd4892"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f0</th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "      <th>f6</th>\n",
              "      <th>f7</th>\n",
              "      <th>f8</th>\n",
              "      <th>f9</th>\n",
              "      <th>...</th>\n",
              "      <th>f90</th>\n",
              "      <th>f91</th>\n",
              "      <th>f92</th>\n",
              "      <th>f93</th>\n",
              "      <th>f94</th>\n",
              "      <th>f95</th>\n",
              "      <th>f96</th>\n",
              "      <th>f97</th>\n",
              "      <th>f98</th>\n",
              "      <th>f99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.306508</td>\n",
              "      <td>2.497590</td>\n",
              "      <td>306.644536</td>\n",
              "      <td>2.647901</td>\n",
              "      <td>0.177850</td>\n",
              "      <td>2.556832</td>\n",
              "      <td>2.699650</td>\n",
              "      <td>2.571593</td>\n",
              "      <td>2.538273</td>\n",
              "      <td>0.134370</td>\n",
              "      <td>...</td>\n",
              "      <td>0.071252</td>\n",
              "      <td>2.444471</td>\n",
              "      <td>0.155260</td>\n",
              "      <td>0.059407</td>\n",
              "      <td>0.144932</td>\n",
              "      <td>0.106419</td>\n",
              "      <td>2.547853</td>\n",
              "      <td>2.590159</td>\n",
              "      <td>0.158881</td>\n",
              "      <td>0.123048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.522450</td>\n",
              "      <td>1.554018</td>\n",
              "      <td>551.743893</td>\n",
              "      <td>1.544529</td>\n",
              "      <td>0.417488</td>\n",
              "      <td>1.562527</td>\n",
              "      <td>1.564000</td>\n",
              "      <td>1.549361</td>\n",
              "      <td>1.532988</td>\n",
              "      <td>0.421892</td>\n",
              "      <td>...</td>\n",
              "      <td>0.112654</td>\n",
              "      <td>1.542509</td>\n",
              "      <td>0.548397</td>\n",
              "      <td>0.119426</td>\n",
              "      <td>0.462015</td>\n",
              "      <td>0.209128</td>\n",
              "      <td>1.558427</td>\n",
              "      <td>1.525091</td>\n",
              "      <td>0.436190</td>\n",
              "      <td>0.264896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-3.797450</td>\n",
              "      <td>-1.223960</td>\n",
              "      <td>-1842.530000</td>\n",
              "      <td>-1.368560</td>\n",
              "      <td>-3.206210</td>\n",
              "      <td>-1.169770</td>\n",
              "      <td>-1.059310</td>\n",
              "      <td>-1.281970</td>\n",
              "      <td>-1.242020</td>\n",
              "      <td>-2.577840</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.676990</td>\n",
              "      <td>-1.217700</td>\n",
              "      <td>-9.761770</td>\n",
              "      <td>-4.666240</td>\n",
              "      <td>-3.101500</td>\n",
              "      <td>-1.276540</td>\n",
              "      <td>-1.584740</td>\n",
              "      <td>-1.254730</td>\n",
              "      <td>-3.993500</td>\n",
              "      <td>-2.783380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.026222</td>\n",
              "      <td>1.186237</td>\n",
              "      <td>43.573400</td>\n",
              "      <td>1.442028</td>\n",
              "      <td>0.019709</td>\n",
              "      <td>1.261038</td>\n",
              "      <td>1.385820</td>\n",
              "      <td>1.333848</td>\n",
              "      <td>1.292163</td>\n",
              "      <td>0.019563</td>\n",
              "      <td>...</td>\n",
              "      <td>0.020496</td>\n",
              "      <td>1.214177</td>\n",
              "      <td>0.018904</td>\n",
              "      <td>0.024483</td>\n",
              "      <td>0.017055</td>\n",
              "      <td>0.025461</td>\n",
              "      <td>1.247888</td>\n",
              "      <td>1.348078</td>\n",
              "      <td>0.013536</td>\n",
              "      <td>0.018105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.097788</td>\n",
              "      <td>2.516500</td>\n",
              "      <td>133.626000</td>\n",
              "      <td>2.634130</td>\n",
              "      <td>0.061586</td>\n",
              "      <td>2.590425</td>\n",
              "      <td>2.801255</td>\n",
              "      <td>2.557985</td>\n",
              "      <td>2.475880</td>\n",
              "      <td>0.058752</td>\n",
              "      <td>...</td>\n",
              "      <td>0.054546</td>\n",
              "      <td>2.386845</td>\n",
              "      <td>0.068906</td>\n",
              "      <td>0.056649</td>\n",
              "      <td>0.063439</td>\n",
              "      <td>0.062151</td>\n",
              "      <td>2.601940</td>\n",
              "      <td>2.682090</td>\n",
              "      <td>0.058058</td>\n",
              "      <td>0.058471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.397184</td>\n",
              "      <td>3.787630</td>\n",
              "      <td>302.262250</td>\n",
              "      <td>3.907640</td>\n",
              "      <td>0.112712</td>\n",
              "      <td>3.813662</td>\n",
              "      <td>3.996913</td>\n",
              "      <td>3.823450</td>\n",
              "      <td>3.804360</td>\n",
              "      <td>0.101046</td>\n",
              "      <td>...</td>\n",
              "      <td>0.091619</td>\n",
              "      <td>3.693872</td>\n",
              "      <td>0.125165</td>\n",
              "      <td>0.088162</td>\n",
              "      <td>0.113114</td>\n",
              "      <td>0.102016</td>\n",
              "      <td>3.820665</td>\n",
              "      <td>3.839520</td>\n",
              "      <td>0.110718</td>\n",
              "      <td>0.104872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>8.781500</td>\n",
              "      <td>6.226720</td>\n",
              "      <td>6119.280000</td>\n",
              "      <td>6.521150</td>\n",
              "      <td>8.265470</td>\n",
              "      <td>6.515070</td>\n",
              "      <td>6.586780</td>\n",
              "      <td>6.258770</td>\n",
              "      <td>6.389670</td>\n",
              "      <td>7.078460</td>\n",
              "      <td>...</td>\n",
              "      <td>6.482940</td>\n",
              "      <td>6.573890</td>\n",
              "      <td>18.412800</td>\n",
              "      <td>10.211800</td>\n",
              "      <td>8.623270</td>\n",
              "      <td>3.657220</td>\n",
              "      <td>6.254360</td>\n",
              "      <td>6.145300</td>\n",
              "      <td>10.767000</td>\n",
              "      <td>5.988110</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 100 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  f0             f1             f2             f3  \\\n",
              "count  600000.000000  600000.000000  600000.000000  600000.000000   \n",
              "mean        0.306508       2.497590     306.644536       2.647901   \n",
              "std         0.522450       1.554018     551.743893       1.544529   \n",
              "min        -3.797450      -1.223960   -1842.530000      -1.368560   \n",
              "25%         0.026222       1.186237      43.573400       1.442028   \n",
              "50%         0.097788       2.516500     133.626000       2.634130   \n",
              "75%         0.397184       3.787630     302.262250       3.907640   \n",
              "max         8.781500       6.226720    6119.280000       6.521150   \n",
              "\n",
              "                  f4             f5             f6             f7  \\\n",
              "count  600000.000000  600000.000000  600000.000000  600000.000000   \n",
              "mean        0.177850       2.556832       2.699650       2.571593   \n",
              "std         0.417488       1.562527       1.564000       1.549361   \n",
              "min        -3.206210      -1.169770      -1.059310      -1.281970   \n",
              "25%         0.019709       1.261038       1.385820       1.333848   \n",
              "50%         0.061586       2.590425       2.801255       2.557985   \n",
              "75%         0.112712       3.813662       3.996913       3.823450   \n",
              "max         8.265470       6.515070       6.586780       6.258770   \n",
              "\n",
              "                  f8             f9  ...            f90            f91  \\\n",
              "count  600000.000000  600000.000000  ...  600000.000000  600000.000000   \n",
              "mean        2.538273       0.134370  ...       0.071252       2.444471   \n",
              "std         1.532988       0.421892  ...       0.112654       1.542509   \n",
              "min        -1.242020      -2.577840  ...      -3.676990      -1.217700   \n",
              "25%         1.292163       0.019563  ...       0.020496       1.214177   \n",
              "50%         2.475880       0.058752  ...       0.054546       2.386845   \n",
              "75%         3.804360       0.101046  ...       0.091619       3.693872   \n",
              "max         6.389670       7.078460  ...       6.482940       6.573890   \n",
              "\n",
              "                 f92            f93            f94            f95  \\\n",
              "count  600000.000000  600000.000000  600000.000000  600000.000000   \n",
              "mean        0.155260       0.059407       0.144932       0.106419   \n",
              "std         0.548397       0.119426       0.462015       0.209128   \n",
              "min        -9.761770      -4.666240      -3.101500      -1.276540   \n",
              "25%         0.018904       0.024483       0.017055       0.025461   \n",
              "50%         0.068906       0.056649       0.063439       0.062151   \n",
              "75%         0.125165       0.088162       0.113114       0.102016   \n",
              "max        18.412800      10.211800       8.623270       3.657220   \n",
              "\n",
              "                 f96            f97            f98            f99  \n",
              "count  600000.000000  600000.000000  600000.000000  600000.000000  \n",
              "mean        2.547853       2.590159       0.158881       0.123048  \n",
              "std         1.558427       1.525091       0.436190       0.264896  \n",
              "min        -1.584740      -1.254730      -3.993500      -2.783380  \n",
              "25%         1.247888       1.348078       0.013536       0.018105  \n",
              "50%         2.601940       2.682090       0.058058       0.058471  \n",
              "75%         3.820665       3.839520       0.110718       0.104872  \n",
              "max         6.254360       6.145300      10.767000       5.988110  \n",
              "\n",
              "[8 rows x 100 columns]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyGhcp2StNcg",
        "outputId": "43fcef47-42c5-40e8-ef57-fdd86de2ce55"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count    600000.000000\n",
              "mean          0.506010\n",
              "std           0.499964\n",
              "min           0.000000\n",
              "25%           0.000000\n",
              "50%           1.000000\n",
              "75%           1.000000\n",
              "max           1.000000\n",
              "Name: target, dtype: float64"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jMx41cxq0q25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "check for null value in X: 0\n",
            "check for null value in y: 0\n"
          ]
        }
      ],
      "source": [
        "X = X.astype(float)\n",
        "y = y.astype(int)\n",
        "\n",
        "X.fillna(X.mean(), inplace=True)\n",
        "y.fillna(y.mean(), inplace=True)\n",
        "\n",
        "print(f'check for null value in X: {X.isnull().sum().sum()}')\n",
        "print(f'check for null value in y: {y.isnull().sum().sum()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "X_test = X_test.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "num_cols = X_train.select_dtypes(['integer', 'float']).columns\n",
        "\n",
        "X_train = pd.DataFrame(scaler.fit_transform(X_train[num_cols]), columns=num_cols)\n",
        "X_test = pd.DataFrame(scaler.fit_transform(X_test[num_cols]), columns=num_cols)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initial Baseline Implementation using KerasClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/v3/ph9z3jvd3h32sv82kzzmklym0000gn/T/ipykernel_74479/4223488330.py:35: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
            "  model = KerasClassifier(build_fn=create_model, verbose=0)\n",
            "2021-11-25 16:08:54.519827: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-25 16:08:54.519827: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-25 16:08:54.520015: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-25 16:08:54.521976: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-25 16:08:54.524233: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-25 16:08:54.525412: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-25 16:08:54.526228: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-25 16:08:54.527048: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best: 0.740409 using {'batch_size': 1024, 'epochs': 1000, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.740409 (0.001741) with: {'batch_size': 1024, 'epochs': 1000, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.739987 (0.002545) with: {'batch_size': 2048, 'epochs': 1000, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "trian score: 0.7581266666666666\n",
            "test score: 0.7430866666666667\n",
            "Precision = 75.69% , recall = 76.89% and f1_score=76.28% of the Logistic Regression Model on the training data.\n",
            "Precision = 74.26% , recall = 75.39% and f1_score=75.00% of the Logistic Regression Model on the validation data.\n",
            "ROC_AUC Score = 75.80%  of Logistic Regression Model on the training data.\n",
            "ROC_AUC Score = 74.29%  of Logistic Regression Model on the validation data.\n"
          ]
        }
      ],
      "source": [
        "# Function to create model, required for KerasClassifier\n",
        "def create_model(optimizer='rmsprop', init='glorot_uniform'):\n",
        "\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(128, input_dim=100, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(64, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(32, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(16, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\t\n",
        "\tmodel.add(Dense(1, kernel_initializer=init, activation='sigmoid'))\n",
        "\n",
        "\t# create model\n",
        "\t#model = Sequential()\n",
        "\t#model.add(Dense(32, input_dim=100, kernel_initializer=init, activation='relu'))\n",
        "\t#model.add(Dense(16, kernel_initializer=init, activation='relu'))\n",
        "\t#model.add(Dense(1, kernel_initializer=init, activation='sigmoid'))\t\n",
        "\t\n",
        "\t# create model\n",
        "\t#model = Sequential()\n",
        "\t#model.add(Dense(128, input_dim=100, kernel_initializer=init, activation='relu'))\n",
        "\t#model.add(Dense(64, kernel_initializer=init, activation='relu'))\n",
        "\t#model.add(Dense(32, kernel_initializer=init, activation='relu'))\n",
        "\t#model.add(Dense(16, kernel_initializer=init, activation='relu'))\n",
        "\t#model.add(Dense(1, kernel_initializer=init, activation='sigmoid'))\n",
        "\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "\treturn model\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "\n",
        "# grid search epochs, batch size and optimizer\n",
        "#optimizers = ['rmsprop', 'adam']\n",
        "#init = ['glorot_uniform', 'normal', 'uniform']\n",
        "#epochs = [100, 150]\n",
        "#batches = [1024, 2048]\n",
        "\n",
        "optimizers = ['adam']\n",
        "init = ['glorot_uniform']\n",
        "epochs = [1000] #[100]\n",
        "batches = [1024, 2048]\n",
        "\n",
        "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
        "\n",
        "y_hat_train = grid.predict(X_train)\n",
        "y_hat_test = grid.predict(X_test)\n",
        "\n",
        "train_score = accuracy_score(y_train, y_hat_train, normalize=False)\n",
        "print(f'trian score: {train_score / y_train.shape[0]}')\n",
        "\n",
        "test_score = accuracy_score(y_test, y_hat_test, normalize=False)\n",
        "print(f'test score: {test_score / y_test.shape[0]}')\n",
        "\n",
        "precision_train_logit_grid = precision_score(y_train, y_hat_train) * 100\n",
        "precision_test_logit_grid = precision_score(y_test, y_hat_test) * 100\n",
        "\n",
        "recall_train_logit_grid = recall_score(y_train, y_hat_train) * 100\n",
        "recall_test_logit_grid = recall_score(y_test, y_hat_test) * 100\n",
        "\n",
        "f1_score_train_logit=f1_score(y_train, y_hat_train) * 100\n",
        "f1_score_test_logit=f1_score(y_test, y_hat_test) * 100\n",
        "\n",
        "auc_score_train_logit_grid = roc_auc_score(y_train, y_hat_train) * 100\n",
        "auc_score_test_logit_grid = roc_auc_score(y_test, y_hat_test) * 100\n",
        "\n",
        "print(\"Precision = {:.2f}% , recall = {:.2f}% and f1_score={:.2f}% of the Logistic Regression Model on the training data.\".format(precision_train_logit_grid, recall_train_logit_grid, f1_score_train_logit))\n",
        "print(\"Precision = {:.2f}% , recall = {:.2f}% and f1_score={:.2f}% of the Logistic Regression Model on the validation data.\".format(precision_test_logit_grid, recall_test_logit_grid, f1_score_test_logit_grid))\n",
        "print(\"ROC_AUC Score = {:.2f}%  of Logistic Regression Model on the training data.\".format(auc_score_train_logit_grid))\n",
        "print(\"ROC_AUC Score = {:.2f}%  of Logistic Regression Model on the validation data.\".format(auc_score_test_logit_grid))\n",
        "\n",
        "# public score: 0.74736 (427th)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define Common Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_keras_sequential_model(optimizer='adam', init='glorot_uniform'):\n",
        "\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(128, input_dim=100, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(64, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(32, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(16, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\t\n",
        "\tmodel.add(Dense(1, kernel_initializer=init, activation='sigmoid'))\n",
        "\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "\n",
        "\treturn model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def report_results(model_name, y_test, y_train, grid_search_model, grid_search_results):\n",
        "\n",
        "\t# summarize results\n",
        "\tprint(\"Best: %f using %s\" % (grid_search_results.best_score_, grid_search_results.best_params_))\n",
        "\tmeans = grid_search_results.cv_results_['mean_test_score']\n",
        "\tstds = grid_search_results.cv_results_['std_test_score']\n",
        "\tparams = grid_search_results.cv_results_['params']\n",
        "\n",
        "\tfor mean, stdev, param in zip(means, stds, params):\n",
        "\t\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
        "\n",
        "\ty_hat_train = grid_search_model.predict(X_train)\n",
        "\ty_hat_test = grid_search_model.predict(X_test)\n",
        "\n",
        "\ttrain_score = accuracy_score(y_train, y_hat_train, normalize=False)\n",
        "\tprint(f'trian score: {train_score / y_train.shape[0]}')\n",
        "\n",
        "\ttest_score = accuracy_score(y_test, y_hat_test, normalize=False)\n",
        "\tprint(f'test score: {test_score / y_test.shape[0]}')\n",
        "\n",
        "\tprecision_train_score = precision_score(y_train, y_hat_train) * 100\n",
        "\tprecision_test_score = precision_score(y_test, y_hat_test) * 100\n",
        "\n",
        "\trecall_train_score = recall_score(y_train, y_hat_train) * 100\n",
        "\trecall_test_score = recall_score(y_test, y_hat_test) * 100\n",
        "\n",
        "\tf1_train_score = f1_score(y_train, y_hat_train) * 100\n",
        "\tf1_test_score = f1_score(y_test, y_hat_test) * 100\n",
        "\n",
        "\tauc_train_score = roc_auc_score(y_train, y_hat_train) * 100\n",
        "\tauc_test_score = roc_auc_score(y_test, y_hat_test) * 100\n",
        "\n",
        "\tprint(\"Precision = {:.2f}% , recall = {:.2f}% and f1_score={:.2f}% of the % model on the training data.\".format(precision_train_score, recall_train_score, f1_train_score, model_name))\n",
        "\tprint(\"Precision = {:.2f}% , recall = {:.2f}% and f1_score={:.2f}% of the % model on the validation data.\".format(precision_test_score, recall_test_score, f1_test_score, model_name))\n",
        "\tprint(\"ROC_AUC Score = {:.2f}%  of the % model on the training data.\".format(auc_train_score, model_name))\n",
        "\tprint(\"ROC_AUC Score = {:.2f}%  of the % model on the validation data.\".format(auc_test_score, model_name))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tune Batch Size and Number of Eposchs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/v3/ph9z3jvd3h32sv82kzzmklym0000gn/T/ipykernel_847/4096780091.py:2: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
            "  model = KerasClassifier(build_fn=create_keras_sequential_model, verbose=0)\n",
            "2021-11-26 16:19:31.204977: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 16:19:31.205013: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 16:19:31.205020: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 16:19:31.205074: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 16:19:31.205131: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 16:19:31.205158: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 16:19:31.205342: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 16:19:31.206394: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/Users/kyle/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:702: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n",
            "2021-11-26 16:47:46.147060: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 16:47:48.785714: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 16:47:56.184710: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 16:48:02.622377: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 16:48:04.925174: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 16:48:59.401037: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 16:49:04.971432: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 16:56:14.352325: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 17:33:16.229410: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 17:39:31.423505: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 17:44:30.233627: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 17:44:33.851318: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 17:44:57.369055: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 17:45:01.148339: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 17:46:25.400191: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 17:47:52.903255: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 17:52:33.354252: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 18:05:19.387584: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 18:05:45.593657: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 18:07:30.613071: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 18:11:01.027648: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 18:16:19.500629: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 18:18:43.166753: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 18:20:04.466342: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 18:21:10.834343: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 18:57:15.489856: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "# create model\n",
        "model = KerasClassifier(build_fn=create_keras_sequential_model, verbose=0)\n",
        "\n",
        "# grid search epochs, batch size and optimizer\n",
        "#optimizers = ['rmsprop', 'adam']\n",
        "#init = ['glorot_uniform', 'normal', 'uniform']\n",
        "#epochs = [100, 150]\n",
        "#batches = [1024, 2048]\n",
        "\n",
        "optimizers = ['adam']\n",
        "init = ['glorot_uniform', 'normal', 'uniform']\n",
        "epochs = [100, 200, 300]\n",
        "batches = [1024, 2048]\n",
        "\n",
        "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best: 0.742073 using {'batch_size': 2048, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.741736 (0.002229) with: {'batch_size': 1024, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.740542 (0.002268) with: {'batch_size': 1024, 'epochs': 100, 'init': 'normal', 'optimizer': 'adam'}\n",
            "0.740862 (0.002184) with: {'batch_size': 1024, 'epochs': 100, 'init': 'uniform', 'optimizer': 'adam'}\n",
            "0.741229 (0.002358) with: {'batch_size': 1024, 'epochs': 200, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.740569 (0.002564) with: {'batch_size': 1024, 'epochs': 200, 'init': 'normal', 'optimizer': 'adam'}\n",
            "0.740144 (0.001736) with: {'batch_size': 1024, 'epochs': 200, 'init': 'uniform', 'optimizer': 'adam'}\n",
            "0.740784 (0.002146) with: {'batch_size': 1024, 'epochs': 300, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.740707 (0.001958) with: {'batch_size': 1024, 'epochs': 300, 'init': 'normal', 'optimizer': 'adam'}\n",
            "0.740747 (0.001736) with: {'batch_size': 1024, 'epochs': 300, 'init': 'uniform', 'optimizer': 'adam'}\n",
            "0.742073 (0.002204) with: {'batch_size': 2048, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.740927 (0.002121) with: {'batch_size': 2048, 'epochs': 100, 'init': 'normal', 'optimizer': 'adam'}\n",
            "0.741124 (0.001895) with: {'batch_size': 2048, 'epochs': 100, 'init': 'uniform', 'optimizer': 'adam'}\n",
            "0.741716 (0.002304) with: {'batch_size': 2048, 'epochs': 200, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.741069 (0.002017) with: {'batch_size': 2048, 'epochs': 200, 'init': 'normal', 'optimizer': 'adam'}\n",
            "0.741260 (0.001941) with: {'batch_size': 2048, 'epochs': 200, 'init': 'uniform', 'optimizer': 'adam'}\n",
            "0.741604 (0.002134) with: {'batch_size': 2048, 'epochs': 300, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.740611 (0.002372) with: {'batch_size': 2048, 'epochs': 300, 'init': 'normal', 'optimizer': 'adam'}\n",
            "0.740669 (0.002684) with: {'batch_size': 2048, 'epochs': 300, 'init': 'uniform', 'optimizer': 'adam'}\n",
            "trian score: 0.7528977777777778\n",
            "test score: 0.7439533333333334\n",
            "Precision = 75.15% , recall = 76.43% and f1_score=75.79% of the % model on the training data.\n",
            "Precision = 74.34% , recall = 75.50% and f1_score=74.91% of the % model on the validation data.\n",
            "ROC_AUC Score = 75.28%  of the % model on the training data.\n",
            "ROC_AUC Score = 74.38%  of the % model on the validation data.\n"
          ]
        }
      ],
      "source": [
        "report_results('KerasClassifier', y_test, y_train, grid, grid_result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/v3/ph9z3jvd3h32sv82kzzmklym0000gn/T/ipykernel_847/219623009.py:15: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
            "  model = KerasClassifier(build_fn=create_keras_sequential_model, verbose=0)\n",
            "2021-11-26 19:07:34.831019: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 19:07:34.831102: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 19:07:34.833789: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 19:07:34.835071: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 19:07:34.838053: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "# re-train with best parameter and full set of train dataset\n",
        "\n",
        "X_train, y_train = X, y\n",
        "\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "X_test = X_test.reset_index(drop=True)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "num_cols = X_train.select_dtypes(['integer', 'float']).columns\n",
        "\n",
        "X_train = pd.DataFrame(scaler.fit_transform(X_train[num_cols]), columns=num_cols)\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_keras_sequential_model, verbose=0)\n",
        "\n",
        "optimizers = ['adam']\n",
        "init = ['glorot_uniform']\n",
        "epochs = [100]\n",
        "batches = [2048]\n",
        "\n",
        "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# public score: 0.74736 (427th)\n",
        "\n",
        "# public score: 0.74591 (second run)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best: 0.743600 using {'batch_size': 2048, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.743600 (0.004309) with: {'batch_size': 2048, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "trian score: 0.7510366666666667\n",
            "test score: 0.75208\n",
            "Precision = 75.08% , recall = 76.03% and f1_score=75.55% of the % model on the training data.\n",
            "Precision = 75.22% , recall = 76.11% and f1_score=75.66% of the % model on the validation data.\n",
            "ROC_AUC Score = 75.09%  of the % model on the training data.\n",
            "ROC_AUC Score = 75.20%  of the % model on the validation data.\n"
          ]
        }
      ],
      "source": [
        "report_results('KerasClassifier', y_test, y_train, grid, grid_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Round 2 - Number of Eposchs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/v3/ph9z3jvd3h32sv82kzzmklym0000gn/T/ipykernel_847/2622259205.py:2: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
            "  model = KerasClassifier(build_fn=create_keras_sequential_model, verbose=0)\n",
            "2021-11-26 19:20:43.306463: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 19:20:43.315967: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 19:20:43.316200: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 19:20:43.319287: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 19:20:43.320722: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best: 0.742638 using {'batch_size': 2048, 'epochs': 1000, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.742638 (0.004250) with: {'batch_size': 2048, 'epochs': 1000, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "trian score: 0.75463\n",
            "test score: 0.7556133333333334\n",
            "Precision = 75.39% , recall = 76.47% and f1_score=75.93% of the % model on the training data.\n",
            "Precision = 75.52% , recall = 76.54% and f1_score=76.03% of the % model on the validation data.\n",
            "ROC_AUC Score = 75.45%  of the % model on the training data.\n",
            "ROC_AUC Score = 75.55%  of the % model on the validation data.\n"
          ]
        }
      ],
      "source": [
        "# create model\n",
        "model = KerasClassifier(build_fn=create_keras_sequential_model, verbose=0)\n",
        "\n",
        "# grid search epochs, batch size and optimizer\n",
        "optimizers = ['adam']\n",
        "init = ['glorot_uniform']\n",
        "epochs = [1000]\n",
        "batches = [2048]\n",
        "\n",
        "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "report_results('KerasClassifier', y_test, y_train, grid, grid_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/v3/ph9z3jvd3h32sv82kzzmklym0000gn/T/ipykernel_847/3139461891.py:15: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
            "  model = KerasClassifier(build_fn=create_keras_sequential_model, verbose=0)\n",
            "2021-11-26 20:56:13.537228: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 20:56:13.539045: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 20:56:13.539699: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 20:56:13.544315: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 20:56:13.548724: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "# re-train with best parameter and full set of train dataset\n",
        "\n",
        "X_train, y_train = X, y\n",
        "\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "X_test = X_test.reset_index(drop=True)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "num_cols = X_train.select_dtypes(['integer', 'float']).columns\n",
        "\n",
        "X_train = pd.DataFrame(scaler.fit_transform(X_train[num_cols]), columns=num_cols)\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_keras_sequential_model, verbose=0)\n",
        "\n",
        "optimizers = ['adam']\n",
        "init = ['glorot_uniform']\n",
        "epochs = [1000]\n",
        "batches = [2048]\n",
        "\n",
        "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# public score: ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best: 0.743443 using {'batch_size': 2048, 'epochs': 1000, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.743443 (0.004062) with: {'batch_size': 2048, 'epochs': 1000, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "trian score: 0.75469\n",
            "test score: 0.7556133333333334\n",
            "Precision = 75.45% , recall = 76.37% and f1_score=75.91% of the % model on the training data.\n",
            "Precision = 75.56% , recall = 76.47% and f1_score=76.01% of the % model on the validation data.\n",
            "ROC_AUC Score = 75.46%  of the % model on the training data.\n",
            "ROC_AUC Score = 75.55%  of the % model on the validation data.\n"
          ]
        }
      ],
      "source": [
        "report_results('KerasClassifier', y_test, y_train, grid, grid_result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tune the Training Optimization Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/v3/ph9z3jvd3h32sv82kzzmklym0000gn/T/ipykernel_76605/467874160.py:2: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
            "  model = KerasClassifier(build_fn=create_keras_sequential_model, verbose=0)\n",
            "2021-11-25 23:04:18.145394: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-25 23:04:18.248205: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "# create model\n",
        "model = KerasClassifier(build_fn=create_keras_sequential_model, verbose=0)\n",
        "\n",
        "#optimizers = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
        "optimizers = ['Adamax']\n",
        "init = ['glorot_uniform']\n",
        "epochs = [100]\n",
        "batches = [2048]\n",
        "\n",
        "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best: 0.744527 using {'batch_size': 2048, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'Adamax'}\n",
            "0.744527 (0.003838) with: {'batch_size': 2048, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'Adamax'}\n",
            "trian score: 0.7519483333333333\n",
            "test score: 0.7527933333333333\n",
            "Precision = 75.20% , recall = 76.06% and f1_score=75.63% of the % model on the training data.\n",
            "Precision = 75.32% , recall = 76.12% and f1_score=75.72% of the % model on the validation data.\n",
            "ROC_AUC Score = 75.18%  of the % model on the training data.\n",
            "ROC_AUC Score = 75.27%  of the % model on the validation data.\n"
          ]
        }
      ],
      "source": [
        "report_results('KerasClassifier', y_test, y_hat_test, y_train, y_hat_train, grid, grid_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/v3/ph9z3jvd3h32sv82kzzmklym0000gn/T/ipykernel_76605/3837577376.py:15: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
            "  model = KerasClassifier(build_fn=create_keras_sequential_model, verbose=0)\n",
            "2021-11-25 23:19:55.142051: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-25 23:19:55.142488: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-25 23:19:55.153509: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-25 23:19:55.154932: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-25 23:19:55.156289: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "# re-train with best parameter\n",
        "\n",
        "X_train, y_train = X, y\n",
        "\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "X_test = X_test.reset_index(drop=True)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "num_cols = X_train.select_dtypes(['integer', 'float']).columns\n",
        "\n",
        "X_train = pd.DataFrame(scaler.fit_transform(X_train[num_cols]), columns=num_cols)\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_keras_sequential_model, verbose=0)\n",
        "\n",
        "optimizers = ['Adamax']\n",
        "init = ['glorot_uniform']\n",
        "epochs = [100]\n",
        "batches = [2048]\n",
        "\n",
        "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# public score: 0.74599"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tune Network Weight Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/v3/ph9z3jvd3h32sv82kzzmklym0000gn/T/ipykernel_76605/2345569135.py:2: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
            "  model = KerasClassifier(build_fn=create_keras_sequential_model, verbose=0)\n",
            "/Users/kyle/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:702: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  warnings.warn(\n",
            "2021-11-26 00:23:42.312301: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 00:23:42.316979: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 00:23:42.319685: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 00:23:42.840614: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 00:23:46.541463: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 00:24:09.357765: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 00:24:25.677908: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 00:25:07.316533: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 00:53:46.803624: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "# create model\n",
        "model = KerasClassifier(build_fn=create_keras_sequential_model, verbose=0)\n",
        "\n",
        "#optimizers = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
        "optimizers = ['Adamax']\n",
        "init = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
        "epochs = [100]\n",
        "batches = [2048]\n",
        "learn_rate = [0.001, 0.01]\n",
        "momentum = [0.0, 0.2, 0.4]\n",
        "\n",
        "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "#grid_result = grid.fit(X_train, y_train)\n",
        "grid_result = grid.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best: 0.744352 using {'batch_size': 2048, 'epochs': 100, 'init': 'he_uniform', 'optimizer': 'Adamax'}\n",
            "0.742848 (0.004295) with: {'batch_size': 2048, 'epochs': 100, 'init': 'uniform', 'optimizer': 'Adamax'}\n",
            "0.743383 (0.003994) with: {'batch_size': 2048, 'epochs': 100, 'init': 'lecun_uniform', 'optimizer': 'Adamax'}\n",
            "0.743197 (0.004248) with: {'batch_size': 2048, 'epochs': 100, 'init': 'normal', 'optimizer': 'Adamax'}\n",
            "0.460970 (0.027027) with: {'batch_size': 2048, 'epochs': 100, 'init': 'zero', 'optimizer': 'Adamax'}\n",
            "0.743533 (0.004100) with: {'batch_size': 2048, 'epochs': 100, 'init': 'glorot_normal', 'optimizer': 'Adamax'}\n",
            "0.743728 (0.004226) with: {'batch_size': 2048, 'epochs': 100, 'init': 'glorot_uniform', 'optimizer': 'Adamax'}\n",
            "0.744147 (0.004041) with: {'batch_size': 2048, 'epochs': 100, 'init': 'he_normal', 'optimizer': 'Adamax'}\n",
            "0.744352 (0.004369) with: {'batch_size': 2048, 'epochs': 100, 'init': 'he_uniform', 'optimizer': 'Adamax'}\n",
            "trian score: 0.75103\n",
            "test score: 0.75176\n",
            "Precision = 75.03% , recall = 76.13% and f1_score=75.58% of the % model on the training data.\n",
            "Precision = 75.13% , recall = 76.20% and f1_score=75.66% of the % model on the validation data.\n",
            "ROC_AUC Score = 75.09%  of the % model on the training data.\n",
            "ROC_AUC Score = 75.16%  of the % model on the validation data.\n"
          ]
        }
      ],
      "source": [
        "report_results('KerasClassifier', y_test, y_hat_test, y_train, y_hat_train, grid, grid_result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/v3/ph9z3jvd3h32sv82kzzmklym0000gn/T/ipykernel_76605/184325068.py:15: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
            "  model = KerasClassifier(build_fn=create_keras_sequential_model, verbose=0)\n",
            "2021-11-26 07:46:38.863666: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 07:46:38.863796: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 07:46:38.863824: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 07:46:38.864180: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 07:46:38.865912: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "# re-train with best parameter\n",
        "\n",
        "X_train, y_train = X, y\n",
        "\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "X_test = X_test.reset_index(drop=True)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "num_cols = X_train.select_dtypes(['integer', 'float']).columns\n",
        "\n",
        "X_train = pd.DataFrame(scaler.fit_transform(X_train[num_cols]), columns=num_cols)\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_keras_sequential_model, verbose=0)\n",
        "\n",
        "optimizers = ['Adamax']\n",
        "init = ['he_uniform']\n",
        "epochs = [100]\n",
        "batches = [2048]\n",
        "\n",
        "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# public score: 0.74599"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Tune Learning Rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/v3/ph9z3jvd3h32sv82kzzmklym0000gn/T/ipykernel_76605/3938506379.py:19: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
            "  model = KerasClassifier(build_fn=create_learning_rate_model, verbose=0)\n",
            "2021-11-26 08:22:11.361209: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 08:22:11.361474: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 08:22:11.363451: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 08:22:11.364469: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 08:22:11.364604: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 08:22:11.364612: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 08:22:11.364604: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 08:22:11.375605: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best: 0.744832 using {'batch_size': 2048, 'epochs': 100, 'init': 'glorot_uniform', 'learn_rate': 0.0001, 'optimizer': 'adam'}\n",
            "0.744832 (0.004127) with: {'batch_size': 2048, 'epochs': 100, 'init': 'glorot_uniform', 'learn_rate': 0.0001, 'optimizer': 'adam'}\n",
            "0.743538 (0.003970) with: {'batch_size': 2048, 'epochs': 100, 'init': 'glorot_uniform', 'learn_rate': 0.001, 'optimizer': 'adam'}\n",
            "0.732608 (0.004847) with: {'batch_size': 2048, 'epochs': 100, 'init': 'glorot_uniform', 'learn_rate': 0.01, 'optimizer': 'adam'}\n",
            "trian score: 0.7494466666666667\n",
            "test score: 0.7503733333333333\n",
            "Precision = 74.93% , recall = 75.88% and f1_score=75.40% of the % model on the training data.\n",
            "Precision = 75.06% , recall = 75.93% and f1_score=75.49% of the % model on the validation data.\n",
            "ROC_AUC Score = 74.93%  of the % model on the training data.\n",
            "ROC_AUC Score = 75.03%  of the % model on the validation data.\n"
          ]
        }
      ],
      "source": [
        "def create_learning_rate_model(optimizer='adam', init='glorot_uniform', learn_rate=0.001):\n",
        "\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(128, input_dim=100, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(64, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(32, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(16, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\t\n",
        "\tmodel.add(Dense(1, kernel_initializer=init, activation='sigmoid'))\n",
        "\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=learn_rate), metrics=['accuracy'])\n",
        "\n",
        "\treturn model\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_learning_rate_model, verbose=0)\n",
        "\n",
        "# grid search epochs, batch size and optimizer\n",
        "optimizers = ['adam']\n",
        "init = ['glorot_uniform']\n",
        "epochs = [100]\n",
        "batches = [2048]\n",
        "# Learning rate controls how much to update the weight at the end of each batch \n",
        "learn_rate = [0.0001, 0.001, 0.01]\n",
        "# and the momentum controls how much to let the previous update influence the current weight update\n",
        "momentum = [0.0, 0.2, 0.4] # ingnore for now\n",
        "\n",
        "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init, learn_rate=learn_rate)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "report_results('KerasClassifier', y_test, y_hat_test, y_train, y_hat_train, grid, grid_result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/v3/ph9z3jvd3h32sv82kzzmklym0000gn/T/ipykernel_76605/3824939399.py:32: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
            "  model = KerasClassifier(build_fn=create_keras_sequential_model, verbose=0)\n",
            "2021-11-26 09:16:22.577960: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 09:16:22.578015: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 09:16:22.578014: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 09:16:22.581560: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-26 09:16:22.583430: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "# re-train with best parameter\n",
        "\n",
        "def create_learning_rate_model(optimizer='adam', init='glorot_uniform', learn_rate=0.0001):\n",
        "\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(128, input_dim=100, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(64, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(32, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(16, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\t\n",
        "\tmodel.add(Dense(1, kernel_initializer=init, activation='sigmoid'))\n",
        "\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=learn_rate), metrics=['accuracy'])\n",
        "\n",
        "\treturn model\n",
        "\n",
        "X_train, y_train = X, y\n",
        "\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "X_test = X_test.reset_index(drop=True)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "num_cols = X_train.select_dtypes(['integer', 'float']).columns\n",
        "\n",
        "X_train = pd.DataFrame(scaler.fit_transform(X_train[num_cols]), columns=num_cols)\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_keras_sequential_model, verbose=0)\n",
        "\n",
        "optimizers = ['adam']\n",
        "init = ['glorot_uniform']\n",
        "epochs = [100]\n",
        "batches = [2048]\n",
        "\n",
        "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# public score: 0.74591"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_test_norm = pd.DataFrame(scaler.transform(test_data[num_cols]), columns = num_cols)\n",
        "\n",
        "test_predict = grid.predict_proba(data_test_norm)[::,1]\n",
        "test_predict = test_predict.astype(float)\n",
        "array = np.array(test_predict).tolist()\n",
        "df = pd.DataFrame(test_data['id'])\n",
        "df['id'] = df['id'].astype(int)\n",
        "df['target'] = np.array(array)\n",
        "df.to_csv('results/keras_results.csv', sep=',', encoding='utf-8', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Users/kyle/Documents/github-data-research-team/kaggle-competitions/tabular-playground-series-nov-2021/results\n",
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /Users/kyle/.kaggle/kaggle.json'\n",
            "100%|███████████████████████████████████████| 13.6M/13.6M [00:21<00:00, 674kB/s]\n",
            "Successfully submitted to Tabular Playground Series - Nov 2021/Users/kyle/Documents/github-data-research-team/kaggle-competitions/tabular-playground-series-nov-2021\n"
          ]
        }
      ],
      "source": [
        "%cd results/\n",
        "\n",
        "!kaggle  competitions  submit -c tabular-playground-series-nov-2021 -f keras_results.csv -m \"keras implementation\"\n",
        "\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /Users/kyle/.kaggle/kaggle.json'\n",
            "fileName           date                 description                         status    publicScore  privateScore  \n",
            "-----------------  -------------------  ----------------------------------  --------  -----------  ------------  \n",
            "keras_results.csv  2021-11-27 06:36:55  keras implementation                complete  0.74616      None          \n",
            "keras_results.csv  2021-11-27 00:15:06  keras implementation                complete  0.74591      None          \n",
            "keras_results.csv  2021-11-26 06:57:18  keras implementation                complete  0.74599      None          \n",
            "keras_results.csv  2021-11-26 05:34:15  keras implementation                complete  0.74736      None          \n",
            "keras_results.csv  2021-11-26 05:08:25  keras implementation                complete  0.73143      None          \n",
            "keras_results.csv  2021-11-26 02:27:10  KerasClassfier baseline submission  complete  0.73143      None          \n",
            "keras_results.csv  2021-11-26 00:05:33  KerasClassfier baseline submission  complete  0.73289      None          \n",
            "keras_results.csv  2021-11-25 22:02:00  KerasClassfier baseline submission  complete  0.70475      None          \n",
            "keras_results.csv  2021-11-25 18:39:49  KerasClassfier baseline submission  complete  0.50284      None          \n",
            "keras_results.csv  2021-11-25 18:00:42  KerasClassfier baseline submission  complete  0.50297      None          \n"
          ]
        }
      ],
      "source": [
        "!kaggle competitions submissions -c tabular-playground-series-nov-2021 -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "keras_playground_nov_2021.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
