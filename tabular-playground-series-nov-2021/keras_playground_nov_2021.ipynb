{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "v7L8ZT2almwl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "import matplotlib as mplt\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_curve, roc_auc_score, f1_score\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.optimizers import Adam\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "lbqW8TTGp4hP"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "NBgD1CYTqTsR"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv('Data/train.csv')\n",
        "test_data = pd.read_csv('Data/test.csv')\n",
        "submission_file = pd.read_csv('Data/sample_submission.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "uQl7kDa8qvxK"
      },
      "outputs": [],
      "source": [
        "train_data.drop(columns=['id'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlErXZC-qx2w",
        "outputId": "a6a3b783-a4b2-4d56-a2ae-d5933c79fdce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(600000, 101)\n",
            "(540000, 101)\n"
          ]
        }
      ],
      "source": [
        "print(train_data.shape)\n",
        "print(test_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "FKLnnrkmq0OG"
      },
      "outputs": [],
      "source": [
        "X, y = train_data.drop(columns = ['target']), train_data['target']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Ilm-eJVq3Sz",
        "outputId": "dc1c720b-d5a9-49e5-dbf3-1fa190ddf9a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "execution_count": 65,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dtypes = train_data.dtypes\n",
        "dtypes = dtypes[dtypes != 'object']\n",
        "features = list(set(dtypes.index) - set(['target']))\n",
        "\n",
        "len(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "YTGSonwyq6fu",
        "outputId": "87363f93-8c1f-492e-a2c4-225c4596344f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f0</th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "      <th>f6</th>\n",
              "      <th>f7</th>\n",
              "      <th>f8</th>\n",
              "      <th>f9</th>\n",
              "      <th>...</th>\n",
              "      <th>f91</th>\n",
              "      <th>f92</th>\n",
              "      <th>f93</th>\n",
              "      <th>f94</th>\n",
              "      <th>f95</th>\n",
              "      <th>f96</th>\n",
              "      <th>f97</th>\n",
              "      <th>f98</th>\n",
              "      <th>f99</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.106643</td>\n",
              "      <td>3.59437</td>\n",
              "      <td>132.8040</td>\n",
              "      <td>3.18428</td>\n",
              "      <td>0.081971</td>\n",
              "      <td>1.18859</td>\n",
              "      <td>3.73238</td>\n",
              "      <td>2.266270</td>\n",
              "      <td>2.09959</td>\n",
              "      <td>0.012330</td>\n",
              "      <td>...</td>\n",
              "      <td>1.09862</td>\n",
              "      <td>0.013331</td>\n",
              "      <td>-0.011715</td>\n",
              "      <td>0.052759</td>\n",
              "      <td>0.065400</td>\n",
              "      <td>4.211250</td>\n",
              "      <td>1.97877</td>\n",
              "      <td>0.085974</td>\n",
              "      <td>0.240496</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.125021</td>\n",
              "      <td>1.67336</td>\n",
              "      <td>76.5336</td>\n",
              "      <td>3.37825</td>\n",
              "      <td>0.099400</td>\n",
              "      <td>5.09366</td>\n",
              "      <td>1.27562</td>\n",
              "      <td>-0.471318</td>\n",
              "      <td>4.54594</td>\n",
              "      <td>0.037706</td>\n",
              "      <td>...</td>\n",
              "      <td>3.46017</td>\n",
              "      <td>0.017054</td>\n",
              "      <td>0.124863</td>\n",
              "      <td>0.154064</td>\n",
              "      <td>0.606848</td>\n",
              "      <td>-0.267928</td>\n",
              "      <td>2.57786</td>\n",
              "      <td>-0.020877</td>\n",
              "      <td>0.024719</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.036330</td>\n",
              "      <td>1.49747</td>\n",
              "      <td>233.5460</td>\n",
              "      <td>2.19435</td>\n",
              "      <td>0.026914</td>\n",
              "      <td>3.12694</td>\n",
              "      <td>5.05687</td>\n",
              "      <td>3.849460</td>\n",
              "      <td>1.80187</td>\n",
              "      <td>0.056995</td>\n",
              "      <td>...</td>\n",
              "      <td>4.88300</td>\n",
              "      <td>0.085222</td>\n",
              "      <td>0.032396</td>\n",
              "      <td>0.116092</td>\n",
              "      <td>-0.001688</td>\n",
              "      <td>-0.520069</td>\n",
              "      <td>2.14112</td>\n",
              "      <td>0.124464</td>\n",
              "      <td>0.148209</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.014077</td>\n",
              "      <td>0.24600</td>\n",
              "      <td>779.9670</td>\n",
              "      <td>1.89064</td>\n",
              "      <td>0.006948</td>\n",
              "      <td>1.53112</td>\n",
              "      <td>2.69800</td>\n",
              "      <td>4.517330</td>\n",
              "      <td>4.50332</td>\n",
              "      <td>0.123494</td>\n",
              "      <td>...</td>\n",
              "      <td>3.47439</td>\n",
              "      <td>-0.017103</td>\n",
              "      <td>-0.008100</td>\n",
              "      <td>0.062013</td>\n",
              "      <td>0.041193</td>\n",
              "      <td>0.511657</td>\n",
              "      <td>1.96860</td>\n",
              "      <td>0.040017</td>\n",
              "      <td>0.044873</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.003259</td>\n",
              "      <td>3.71542</td>\n",
              "      <td>156.1280</td>\n",
              "      <td>2.14772</td>\n",
              "      <td>0.018284</td>\n",
              "      <td>2.09859</td>\n",
              "      <td>4.15492</td>\n",
              "      <td>-0.038236</td>\n",
              "      <td>3.37145</td>\n",
              "      <td>0.034166</td>\n",
              "      <td>...</td>\n",
              "      <td>1.91059</td>\n",
              "      <td>-0.042943</td>\n",
              "      <td>0.105616</td>\n",
              "      <td>0.125072</td>\n",
              "      <td>0.037509</td>\n",
              "      <td>1.043790</td>\n",
              "      <td>1.07481</td>\n",
              "      <td>-0.012819</td>\n",
              "      <td>0.072798</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 101 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         f0       f1        f2       f3        f4       f5       f6        f7  \\\n",
              "0  0.106643  3.59437  132.8040  3.18428  0.081971  1.18859  3.73238  2.266270   \n",
              "1  0.125021  1.67336   76.5336  3.37825  0.099400  5.09366  1.27562 -0.471318   \n",
              "2  0.036330  1.49747  233.5460  2.19435  0.026914  3.12694  5.05687  3.849460   \n",
              "3 -0.014077  0.24600  779.9670  1.89064  0.006948  1.53112  2.69800  4.517330   \n",
              "4 -0.003259  3.71542  156.1280  2.14772  0.018284  2.09859  4.15492 -0.038236   \n",
              "\n",
              "        f8        f9  ...      f91       f92       f93       f94       f95  \\\n",
              "0  2.09959  0.012330  ...  1.09862  0.013331 -0.011715  0.052759  0.065400   \n",
              "1  4.54594  0.037706  ...  3.46017  0.017054  0.124863  0.154064  0.606848   \n",
              "2  1.80187  0.056995  ...  4.88300  0.085222  0.032396  0.116092 -0.001688   \n",
              "3  4.50332  0.123494  ...  3.47439 -0.017103 -0.008100  0.062013  0.041193   \n",
              "4  3.37145  0.034166  ...  1.91059 -0.042943  0.105616  0.125072  0.037509   \n",
              "\n",
              "        f96      f97       f98       f99  target  \n",
              "0  4.211250  1.97877  0.085974  0.240496       0  \n",
              "1 -0.267928  2.57786 -0.020877  0.024719       0  \n",
              "2 -0.520069  2.14112  0.124464  0.148209       0  \n",
              "3  0.511657  1.96860  0.040017  0.044873       0  \n",
              "4  1.043790  1.07481 -0.012819  0.072798       1  \n",
              "\n",
              "[5 rows x 101 columns]"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "EnnOgS9Rrfol",
        "outputId": "c4da73b4-1a40-4732-aacb-7936c1dd4892"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f0</th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>f5</th>\n",
              "      <th>f6</th>\n",
              "      <th>f7</th>\n",
              "      <th>f8</th>\n",
              "      <th>f9</th>\n",
              "      <th>...</th>\n",
              "      <th>f90</th>\n",
              "      <th>f91</th>\n",
              "      <th>f92</th>\n",
              "      <th>f93</th>\n",
              "      <th>f94</th>\n",
              "      <th>f95</th>\n",
              "      <th>f96</th>\n",
              "      <th>f97</th>\n",
              "      <th>f98</th>\n",
              "      <th>f99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "      <td>600000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.306508</td>\n",
              "      <td>2.497590</td>\n",
              "      <td>306.644536</td>\n",
              "      <td>2.647901</td>\n",
              "      <td>0.177850</td>\n",
              "      <td>2.556832</td>\n",
              "      <td>2.699650</td>\n",
              "      <td>2.571593</td>\n",
              "      <td>2.538273</td>\n",
              "      <td>0.134370</td>\n",
              "      <td>...</td>\n",
              "      <td>0.071252</td>\n",
              "      <td>2.444471</td>\n",
              "      <td>0.155260</td>\n",
              "      <td>0.059407</td>\n",
              "      <td>0.144932</td>\n",
              "      <td>0.106419</td>\n",
              "      <td>2.547853</td>\n",
              "      <td>2.590159</td>\n",
              "      <td>0.158881</td>\n",
              "      <td>0.123048</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.522450</td>\n",
              "      <td>1.554018</td>\n",
              "      <td>551.743893</td>\n",
              "      <td>1.544529</td>\n",
              "      <td>0.417488</td>\n",
              "      <td>1.562527</td>\n",
              "      <td>1.564000</td>\n",
              "      <td>1.549361</td>\n",
              "      <td>1.532988</td>\n",
              "      <td>0.421892</td>\n",
              "      <td>...</td>\n",
              "      <td>0.112654</td>\n",
              "      <td>1.542509</td>\n",
              "      <td>0.548397</td>\n",
              "      <td>0.119426</td>\n",
              "      <td>0.462015</td>\n",
              "      <td>0.209128</td>\n",
              "      <td>1.558427</td>\n",
              "      <td>1.525091</td>\n",
              "      <td>0.436190</td>\n",
              "      <td>0.264896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-3.797450</td>\n",
              "      <td>-1.223960</td>\n",
              "      <td>-1842.530000</td>\n",
              "      <td>-1.368560</td>\n",
              "      <td>-3.206210</td>\n",
              "      <td>-1.169770</td>\n",
              "      <td>-1.059310</td>\n",
              "      <td>-1.281970</td>\n",
              "      <td>-1.242020</td>\n",
              "      <td>-2.577840</td>\n",
              "      <td>...</td>\n",
              "      <td>-3.676990</td>\n",
              "      <td>-1.217700</td>\n",
              "      <td>-9.761770</td>\n",
              "      <td>-4.666240</td>\n",
              "      <td>-3.101500</td>\n",
              "      <td>-1.276540</td>\n",
              "      <td>-1.584740</td>\n",
              "      <td>-1.254730</td>\n",
              "      <td>-3.993500</td>\n",
              "      <td>-2.783380</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.026222</td>\n",
              "      <td>1.186237</td>\n",
              "      <td>43.573400</td>\n",
              "      <td>1.442028</td>\n",
              "      <td>0.019709</td>\n",
              "      <td>1.261038</td>\n",
              "      <td>1.385820</td>\n",
              "      <td>1.333848</td>\n",
              "      <td>1.292163</td>\n",
              "      <td>0.019563</td>\n",
              "      <td>...</td>\n",
              "      <td>0.020496</td>\n",
              "      <td>1.214177</td>\n",
              "      <td>0.018904</td>\n",
              "      <td>0.024483</td>\n",
              "      <td>0.017055</td>\n",
              "      <td>0.025461</td>\n",
              "      <td>1.247888</td>\n",
              "      <td>1.348078</td>\n",
              "      <td>0.013536</td>\n",
              "      <td>0.018105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.097788</td>\n",
              "      <td>2.516500</td>\n",
              "      <td>133.626000</td>\n",
              "      <td>2.634130</td>\n",
              "      <td>0.061586</td>\n",
              "      <td>2.590425</td>\n",
              "      <td>2.801255</td>\n",
              "      <td>2.557985</td>\n",
              "      <td>2.475880</td>\n",
              "      <td>0.058752</td>\n",
              "      <td>...</td>\n",
              "      <td>0.054546</td>\n",
              "      <td>2.386845</td>\n",
              "      <td>0.068906</td>\n",
              "      <td>0.056649</td>\n",
              "      <td>0.063439</td>\n",
              "      <td>0.062151</td>\n",
              "      <td>2.601940</td>\n",
              "      <td>2.682090</td>\n",
              "      <td>0.058058</td>\n",
              "      <td>0.058471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.397184</td>\n",
              "      <td>3.787630</td>\n",
              "      <td>302.262250</td>\n",
              "      <td>3.907640</td>\n",
              "      <td>0.112712</td>\n",
              "      <td>3.813662</td>\n",
              "      <td>3.996913</td>\n",
              "      <td>3.823450</td>\n",
              "      <td>3.804360</td>\n",
              "      <td>0.101046</td>\n",
              "      <td>...</td>\n",
              "      <td>0.091619</td>\n",
              "      <td>3.693872</td>\n",
              "      <td>0.125165</td>\n",
              "      <td>0.088162</td>\n",
              "      <td>0.113114</td>\n",
              "      <td>0.102016</td>\n",
              "      <td>3.820665</td>\n",
              "      <td>3.839520</td>\n",
              "      <td>0.110718</td>\n",
              "      <td>0.104872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>8.781500</td>\n",
              "      <td>6.226720</td>\n",
              "      <td>6119.280000</td>\n",
              "      <td>6.521150</td>\n",
              "      <td>8.265470</td>\n",
              "      <td>6.515070</td>\n",
              "      <td>6.586780</td>\n",
              "      <td>6.258770</td>\n",
              "      <td>6.389670</td>\n",
              "      <td>7.078460</td>\n",
              "      <td>...</td>\n",
              "      <td>6.482940</td>\n",
              "      <td>6.573890</td>\n",
              "      <td>18.412800</td>\n",
              "      <td>10.211800</td>\n",
              "      <td>8.623270</td>\n",
              "      <td>3.657220</td>\n",
              "      <td>6.254360</td>\n",
              "      <td>6.145300</td>\n",
              "      <td>10.767000</td>\n",
              "      <td>5.988110</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 100 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                  f0             f1             f2             f3  \\\n",
              "count  600000.000000  600000.000000  600000.000000  600000.000000   \n",
              "mean        0.306508       2.497590     306.644536       2.647901   \n",
              "std         0.522450       1.554018     551.743893       1.544529   \n",
              "min        -3.797450      -1.223960   -1842.530000      -1.368560   \n",
              "25%         0.026222       1.186237      43.573400       1.442028   \n",
              "50%         0.097788       2.516500     133.626000       2.634130   \n",
              "75%         0.397184       3.787630     302.262250       3.907640   \n",
              "max         8.781500       6.226720    6119.280000       6.521150   \n",
              "\n",
              "                  f4             f5             f6             f7  \\\n",
              "count  600000.000000  600000.000000  600000.000000  600000.000000   \n",
              "mean        0.177850       2.556832       2.699650       2.571593   \n",
              "std         0.417488       1.562527       1.564000       1.549361   \n",
              "min        -3.206210      -1.169770      -1.059310      -1.281970   \n",
              "25%         0.019709       1.261038       1.385820       1.333848   \n",
              "50%         0.061586       2.590425       2.801255       2.557985   \n",
              "75%         0.112712       3.813662       3.996913       3.823450   \n",
              "max         8.265470       6.515070       6.586780       6.258770   \n",
              "\n",
              "                  f8             f9  ...            f90            f91  \\\n",
              "count  600000.000000  600000.000000  ...  600000.000000  600000.000000   \n",
              "mean        2.538273       0.134370  ...       0.071252       2.444471   \n",
              "std         1.532988       0.421892  ...       0.112654       1.542509   \n",
              "min        -1.242020      -2.577840  ...      -3.676990      -1.217700   \n",
              "25%         1.292163       0.019563  ...       0.020496       1.214177   \n",
              "50%         2.475880       0.058752  ...       0.054546       2.386845   \n",
              "75%         3.804360       0.101046  ...       0.091619       3.693872   \n",
              "max         6.389670       7.078460  ...       6.482940       6.573890   \n",
              "\n",
              "                 f92            f93            f94            f95  \\\n",
              "count  600000.000000  600000.000000  600000.000000  600000.000000   \n",
              "mean        0.155260       0.059407       0.144932       0.106419   \n",
              "std         0.548397       0.119426       0.462015       0.209128   \n",
              "min        -9.761770      -4.666240      -3.101500      -1.276540   \n",
              "25%         0.018904       0.024483       0.017055       0.025461   \n",
              "50%         0.068906       0.056649       0.063439       0.062151   \n",
              "75%         0.125165       0.088162       0.113114       0.102016   \n",
              "max        18.412800      10.211800       8.623270       3.657220   \n",
              "\n",
              "                 f96            f97            f98            f99  \n",
              "count  600000.000000  600000.000000  600000.000000  600000.000000  \n",
              "mean        2.547853       2.590159       0.158881       0.123048  \n",
              "std         1.558427       1.525091       0.436190       0.264896  \n",
              "min        -1.584740      -1.254730      -3.993500      -2.783380  \n",
              "25%         1.247888       1.348078       0.013536       0.018105  \n",
              "50%         2.601940       2.682090       0.058058       0.058471  \n",
              "75%         3.820665       3.839520       0.110718       0.104872  \n",
              "max         6.254360       6.145300      10.767000       5.988110  \n",
              "\n",
              "[8 rows x 100 columns]"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyGhcp2StNcg",
        "outputId": "43fcef47-42c5-40e8-ef57-fdd86de2ce55"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "count    600000.000000\n",
              "mean          0.506010\n",
              "std           0.499964\n",
              "min           0.000000\n",
              "25%           0.000000\n",
              "50%           1.000000\n",
              "75%           1.000000\n",
              "max           1.000000\n",
              "Name: target, dtype: float64"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "jMx41cxq0q25"
      },
      "outputs": [],
      "source": [
        "X = X.astype(float)\n",
        "y = y.astype(int)\n",
        "\n",
        "X.fillna(X.mean(), inplace=True)\n",
        "y.fillna(y.mean(), inplace=True)\n",
        "\n",
        "print(f'check for null value in X: {X.isnull().sum().sum()}')\n",
        "print(f'check for null value in y: {y.isnull().sum().sum()}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "X_test = X_test.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "num_cols = X_train.select_dtypes(['integer', 'float']).columns\n",
        "\n",
        "X_train = pd.DataFrame(scaler.fit_transform(X_train[num_cols]), columns=num_cols)\n",
        "X_test = pd.DataFrame(scaler.fit_transform(X_test[num_cols]), columns=num_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/v3/ph9z3jvd3h32sv82kzzmklym0000gn/T/ipykernel_74479/1278638531.py:44: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead.\n",
            "  model = KerasClassifier(build_fn=create_model, verbose=0)\n",
            "2021-11-25 10:54:02.492529: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-25 10:54:02.492974: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-25 10:54:02.493914: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-25 10:54:02.494111: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-11-25 10:54:02.500497: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best: 0.708082 using {'batch_size': 2048, 'epochs': 1000, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "0.708082 (0.002871) with: {'batch_size': 2048, 'epochs': 1000, 'init': 'glorot_uniform', 'optimizer': 'adam'}\n",
            "trian score: 0.7860022222222223\n",
            "test score: 0.71152\n",
            "Precision = 78.26% , recall = 79.89% and f1_score=79.07% of the Logistic Regression Model on the training data.\n",
            "Precision = 71.13% , recall = 72.42% and f1_score=75.00% of the Logistic Regression Model on the validation data.\n",
            "ROC_AUC Score = 78.58%  of Logistic Regression Model on the training data.\n",
            "ROC_AUC Score = 71.14%  of Logistic Regression Model on the validation data.\n"
          ]
        }
      ],
      "source": [
        "# Function to create model, required for KerasClassifier\n",
        "def create_model(optimizer='rmsprop', init='glorot_uniform'):\n",
        "\n",
        "\t# create model\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(128, input_dim=100, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(64, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(32, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\n",
        "\tmodel.add(Dense(16, kernel_initializer=init, activation='relu'))\n",
        "\tmodel.add(Dropout(0.2))\t\n",
        "\tmodel.add(Dense(1, kernel_initializer=init, activation='sigmoid'))\n",
        "\n",
        "\t# create model\n",
        "\t#model = Sequential()\n",
        "\t#model.add(Dense(32, input_dim=100, kernel_initializer=init, activation='relu'))\n",
        "\t#model.add(Dense(16, kernel_initializer=init, activation='relu'))\n",
        "\t#model.add(Dense(1, kernel_initializer=init, activation='sigmoid'))\t\n",
        "\t\n",
        "\t# create model\n",
        "\t#model = Sequential()\n",
        "\t#model.add(Dense(128, input_dim=100, kernel_initializer=init, activation='relu'))\n",
        "\t#model.add(Dense(64, kernel_initializer=init, activation='relu'))\n",
        "\t#model.add(Dense(32, kernel_initializer=init, activation='relu'))\n",
        "\t#model.add(Dense(16, kernel_initializer=init, activation='relu'))\n",
        "\t#model.add(Dense(1, kernel_initializer=init, activation='sigmoid'))\n",
        "\n",
        "\t# Compile model\n",
        "\tmodel.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
        "\treturn model\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "\n",
        "# grid search epochs, batch size and optimizer\n",
        "#optimizers = ['rmsprop', 'adam']\n",
        "#init = ['glorot_uniform', 'normal', 'uniform']\n",
        "#epochs = [100, 150]\n",
        "#batches = [1024, 2048]\n",
        "\n",
        "optimizers = ['adam']\n",
        "init = ['glorot_uniform']\n",
        "epochs = [100] #[1000]\n",
        "batches = [1024]\n",
        "\n",
        "param_grid = dict(optimizer=optimizers, epochs=epochs, batch_size=batches, init=init)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=8)\n",
        "grid_result = grid.fit(X_train, y_train)\n",
        "\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
        "\n",
        "y_hat_train = grid.predict(X_train)\n",
        "y_hat_test = grid.predict(X_test)\n",
        "\n",
        "train_score = accuracy_score(y_train, y_hat_train, normalize=False)\n",
        "print(f'trian score: {train_score / y_train.shape[0]}')\n",
        "\n",
        "test_score = accuracy_score(y_test, y_hat_test, normalize=False)\n",
        "print(f'test score: {test_score / y_test.shape[0]}')\n",
        "\n",
        "precision_train_logit_grid = precision_score(y_train, y_hat_train) * 100\n",
        "precision_test_logit_grid = precision_score(y_test, y_hat_test) * 100\n",
        "\n",
        "recall_train_logit_grid = recall_score(y_train, y_hat_train) * 100\n",
        "recall_test_logit_grid = recall_score(y_test, y_hat_test) * 100\n",
        "\n",
        "f1_score_train_logit=f1_score(y_train, y_hat_train) * 100\n",
        "f1_score_test_logit=f1_score(y_test, y_hat_test) * 100\n",
        "\n",
        "auc_score_train_logit_grid = roc_auc_score(y_train, y_hat_train) * 100\n",
        "auc_score_test_logit_grid = roc_auc_score(y_test, y_hat_test) * 100\n",
        "\n",
        "print(\"Precision = {:.2f}% , recall = {:.2f}% and f1_score={:.2f}% of the Logistic Regression Model on the training data.\".format(precision_train_logit_grid, recall_train_logit_grid, f1_score_train_logit))\n",
        "print(\"Precision = {:.2f}% , recall = {:.2f}% and f1_score={:.2f}% of the Logistic Regression Model on the validation data.\".format(precision_test_logit_grid, recall_test_logit_grid, f1_score_test_logit_grid))\n",
        "print(\"ROC_AUC Score = {:.2f}%  of Logistic Regression Model on the training data.\".format(auc_score_train_logit_grid))\n",
        "print(\"ROC_AUC Score = {:.2f}%  of Logistic Regression Model on the validation data.\".format(auc_score_test_logit_grid))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_test_norm = pd.DataFrame(scaler.transform(test_data[num_cols]), columns = num_cols)\n",
        "\n",
        "test_predict = grid.predict_proba(data_test_norm)[::,1]\n",
        "test_predict=test_predict.astype(float)\n",
        "array=np.array(test_predict).tolist()\n",
        "df=pd.DataFrame(test_data['id'])\n",
        "df['id'] = df['id'].astype(int)\n",
        "df['target'] = np.array(array)\n",
        "df.to_csv('results/keras_results.csv', sep=',', encoding='utf-8', index=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "keras_playground_nov_2021.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
